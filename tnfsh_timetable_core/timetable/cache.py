import re
from typing import Dict, Optional
from datetime import datetime
import logging
import asyncio
from pathlib import Path
import json
from aiohttp import client_exceptions
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
    RetryError
)

from tnfsh_timetable_core.abc.cache_abc import BaseCacheABC
from tnfsh_timetable_core.timetable.models import (
    CacheMetadata,
    CachedTimeTable, 
    TimetableSchema
)
from tnfsh_timetable_core.timetable.crawler import TimetableCrawler, FetchError
from tnfsh_timetable_core.utils.logger import get_logger
from pydantic import BaseModel

logger = get_logger(logger_level="INFO")



# å…¨åŸŸè¨˜æ†¶é«”å¿«å–
_memory_cache: Dict[str, CachedTimeTable] = {}

class TimeTableCache(BaseCacheABC):
    """èª²è¡¨çš„å¿«å–å¯¦ä½œï¼Œæ”¯æ´ä¸‰å±¤å¿«å–æ¶æ§‹"""
    
    def __init__(
        self, 
        crawler: Optional[TimetableCrawler] = None,
        cache_dir: Optional[str] = None,
        file_path_template: str = "prebuilt_{target}_{ID}.json"
    ):
        """åˆå§‹åŒ–å¿«å–ç³»çµ±
        
        Args:
            crawler: èª²è¡¨çˆ¬èŸ²å¯¦ä¾‹ï¼Œå¦‚æœæœªæä¾›æœƒå»ºç«‹æ–°çš„å¯¦ä¾‹
            cache_dir: å¿«å–ç›®éŒ„è·¯å¾‘ï¼Œå¦‚æœæœªæä¾›å‰‡ä½¿ç”¨é è¨­è·¯å¾‘
            file_path_template: å¿«å–æª”æ¡ˆåç¨±æ¨¡æ¿ï¼Œå¯ç”¨ {target} åšæ›¿æ›
        """
        self._crawler = crawler or TimetableCrawler()
        self._cache_dir = Path(cache_dir) if cache_dir else Path(__file__).resolve().parent / "cache"
        self._file_path_template = file_path_template
        self._cache_dir.mkdir(exist_ok=True)

    def _get_cache_path(self, target: str, id: str) -> Path:
        """å–å¾—ç›®æ¨™çš„å¿«å–æª”æ¡ˆè·¯å¾‘
        
        Args:
            target: ç›®æ¨™åç¨±ï¼ˆç­ç´šæˆ–æ•™å¸«ï¼‰
            
        Returns:
            Path: å¿«å–æª”æ¡ˆè·¯å¾‘
        """
        # ç¢ºä¿æª”åå®‰å…¨
        safe_target = "".join(c for c in target if c.isalnum() or c in "-_")
        return self._cache_dir / self._file_path_template.format(target=safe_target, id=id)

    async def fetch_from_memory(self, id: str, *args, **kwargs) -> Optional[CachedTimeTable]:
        """å¾å…¨åŸŸè®Šæ•¸å¿«å–å–å¾—èª²è¡¨è³‡æ–™
        
        Args:
            target: ç›®æ¨™åç¨±ï¼ˆç­ç´šæˆ–æ•™å¸«ï¼‰
            
        Returns:
            Optional[CachedTimeTable]: å¿«å–çš„èª²è¡¨è³‡æ–™ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡ç‚º None
        """
        if id in _memory_cache:
            logger.debug(f"âœ¨ å¾è¨˜æ†¶é«”å¿«å–å–å¾—èª²è¡¨ï¼š{id}")
            return _memory_cache[id]
        return None

    async def save_to_memory(self, data: CachedTimeTable, id: str, *args, **kwargs) -> None:
        """å„²å­˜èª²è¡¨è³‡æ–™åˆ°å…¨åŸŸè®Šæ•¸å¿«å–
        
        Args:
            data: è¦å„²å­˜çš„èª²è¡¨è³‡æ–™
            target: ç›®æ¨™åç¨±ï¼ˆç­ç´šæˆ–æ•™å¸«ï¼‰
        """
        _memory_cache[id] = data
        logger.debug(f"âœ¨ å·²æ›´æ–°è¨˜æ†¶é«”å¿«å–ï¼š{id}")

    async def fetch_from_file(self, target: str, id: str, *args, **kwargs) -> Optional[CachedTimeTable]:
        """å¾æª”æ¡ˆå¿«å–å–å¾—èª²è¡¨è³‡æ–™
        
        Args:
            target: ç›®æ¨™åç¨±ï¼ˆç­ç´šæˆ–æ•™å¸«ï¼‰
            
        Returns:
            Optional[CachedTimeTable]: å¿«å–çš„èª²è¡¨è³‡æ–™ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è®€å–å¤±æ•—å‰‡ç‚º None
        """
        cache_path = self._get_cache_path(target, id)
        try:
            if not cache_path.exists():
                return None
                
            with open(cache_path, encoding="utf-8") as f:
                data = json.load(f)
                result = CachedTimeTable.model_validate(data)
                logger.debug(f"ğŸ’¾ å¾æª”æ¡ˆè¼‰å…¥èª²è¡¨å¿«å–ï¼š{id}")
                return result
        except Exception as e:
            logger.error(f"è®€å–å¿«å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    async def save_to_file(self, data: CachedTimeTable, target: str, id: str, *args, **kwargs) -> None:
        """å„²å­˜èª²è¡¨è³‡æ–™åˆ°æª”æ¡ˆå¿«å–
        
        Args:
            data: è¦å„²å­˜çš„èª²è¡¨è³‡æ–™
            target: ç›®æ¨™åç¨±ï¼ˆç­ç´šæˆ–æ•™å¸«ï¼‰
        """
        cache_path = self._get_cache_path(target, id)
        try:
            with open(cache_path, "w", encoding="utf-8") as f:
                json_data = data.model_dump_json(indent=4)
                f.write(json_data)
                logger.debug(f"ğŸ’¾ å·²æ›´æ–°æª”æ¡ˆå¿«å–ï¼š{id}")
        except Exception as e:
            logger.error(f"å„²å­˜å¿«å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise FetchError(f"å„²å­˜å¿«å–æª”æ¡ˆå¤±æ•—: {str(e)}")
    
    @retry(
        retry=retry_if_exception_type((
            FetchError,
            client_exceptions.ClientError,
            client_exceptions.ServerTimeoutError,
            asyncio.TimeoutError
        )),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=5),
        before_sleep=before_sleep_log(logger, logging.WARNING)
    )
    async def fetch_from_source(self, id: str, *args, **kwargs) -> CachedTimeTable:
        """å¾ç¶²è·¯ä¾†æºå–å¾—æœ€æ–°çš„èª²è¡¨è³‡æ–™
        
        Args:
            target: ç›®æ¨™åç¨±ï¼ˆç­ç´šæˆ–æ•™å¸«ï¼‰
            
        Returns:
            CachedTimeTable: åŒ…å«æœ€æ–°èª²è¡¨è³‡æ–™çš„å¿«å–çµæ§‹
        """
        try:
            logger.info(f"ğŸŒ å¾ç¶²è·¯æŠ“å–èª²è¡¨ï¼š{id}")
            timetable = await self._crawler.fetch(id, refresh=kwargs.get('refresh', False))
            
            cached_result = CachedTimeTable(
                metadata=CacheMetadata(cache_fetch_at=datetime.now()),
                data=timetable
            )
            
            return cached_result
            
        except Exception as e:
            error_msg = f"å¾ä¾†æºæŠ“å–èª²è¡¨å¤±æ•— {id}: {str(e)}"
            logger.warning(f"âš ï¸ {error_msg}")
            raise FetchError(error_msg)

    async def fetch(self, target: str, *, refresh: bool = False, **kwargs) -> CachedTimeTable:
        """æ™ºèƒ½ç²å–èª²è¡¨è³‡æ–™ï¼Œè‡ªå‹•è™•ç†ä¸‰å±¤å¿«å–
        
        Args:
            target: ç›®æ¨™åç¨±ï¼ˆç­ç´šæˆ–æ•™å¸«ï¼‰
            refresh: æ˜¯å¦å¼·åˆ¶æ›´æ–°å¿«å–
            **kwargs: å‚³éçµ¦åº•å±¤æ–¹æ³•çš„é¡å¤–åƒæ•¸
            
        Returns:
            CachedTimeTable: å®Œæ•´çš„èª²è¡¨è³‡æ–™
        """
        from tnfsh_timetable_core import TNFSHTimetableCore
        core = TNFSHTimetableCore()
        index = await core.fetch_index(refresh=refresh)
        info = index[target]
        from tnfsh_timetable_core.index.models import TargetInfo
        if isinstance(info, TargetInfo):
            id = info.id
            target = info.target
        elif isinstance(info, list):
            raise FetchError(f"ç›®æ¨™ {target} æœ‰å¤šç¨®å¯èƒ½çš„ IDï¼Œè«‹æŒ‡å®šå…·é«” ID: {info}")
        else:
            raise FetchError(f"ç„¡æ³•è§£æç›®æ¨™ {target} çš„ IDï¼Œè«‹æª¢æŸ¥ç´¢å¼•è³‡æ–™")
        
        if not refresh:
            mem = await self.fetch_from_memory(id)
            if mem:
                return mem
        if not refresh:
            file = await self.fetch_from_file(target, id)
            if file:
                await self.save_to_memory(file, id)
                return file
        net = await self.fetch_from_source(id)
        await self.save_to_file(net, target, id)
        await self.save_to_memory(net, id)
        return net
        

@retry(
    stop=stop_after_attempt(2),  # æ•´é«”æœ€å¤šé‡è©¦ 2 æ¬¡
    retry=retry_if_exception_type(FetchError),  # åªåœ¨ç²å–ç´¢å¼•å¤±æ•—æ™‚é‡è©¦
    wait=wait_exponential(multiplier=1, min=1, max=10),
    before_sleep=before_sleep_log(logger, logging.WARNING)
)
async def preload_all(only_missing: bool = True, max_concurrent: int = 5, delay: float = 0.0):
    """é è¼‰å…¥æ‰€æœ‰èª²è¡¨ï¼ŒåŠ å…¥ä½µç™¼ä¸Šé™èˆ‡å»¶é²æ§åˆ¶
    
    Args:
        only_missing: æ˜¯å¦åªè¼‰å…¥ç¼ºå°‘çš„èª²è¡¨ï¼Œé è¨­ç‚º True
        max_concurrent: æœ€å¤§ä½µç™¼è«‹æ±‚æ•¸é‡ï¼Œé è¨­ç‚º 5
        delay: æ¯ç­†è«‹æ±‚å‰çš„å»¶é²ç§’æ•¸ï¼Œé è¨­ç‚º 0
    """
    from tnfsh_timetable_core import TNFSHTimetableCore
    import asyncio

    try:
        # ç²å–ç´¢å¼•
        core = TNFSHTimetableCore()
        index = await core.fetch_index(refresh=True)
        
        if not index.reverse_index:
            error_msg = "âŒ ç„¡æ³•ç²å–èª²è¡¨ç´¢å¼•"
            logger.error(error_msg)
            raise FetchError(error_msg)

        ids = index.get_all_ids()
        logger.info(f"ğŸ”„ é–‹å§‹é è¼‰å…¥æ‰€æœ‰èª²è¡¨ï¼Œå…± {len(ids)} é …ï¼Œå»¶é²ï¼š{delay} ç§’ï¼Œä½µç™¼ä¸Šé™ï¼š{max_concurrent}")

        cache = TimeTableCache()
        semaphore = asyncio.Semaphore(max_concurrent)

        async def process(id: str):
            # æª¢æŸ¥æ˜¯å¦å·²æœ‰å¿«å–
            if only_missing:
                cached = await cache.fetch_from_memory(id) or await cache.fetch_from_file(id)
                if cached:
                    logger.debug(f"âš¡ å¿«å–å·²å­˜åœ¨ï¼Œç•¥éï¼š{id}")
                    return
                
            # å…§éƒ¨é‡è©¦å‡½æ•¸
            @retry(
                stop=stop_after_attempt(3),  # å–®å€‹ç›®æ¨™æœ€å¤šé‡è©¦ 3 æ¬¡
                retry=retry_if_exception_type((
                    FetchError,
                    client_exceptions.ClientError,
                    client_exceptions.ServerTimeoutError,
                    asyncio.TimeoutError
                )),
                wait=wait_exponential(multiplier=1, min=1, max=5),
                before_sleep=before_sleep_log(logger, logging.DEBUG)
            )
            async def _fetch_with_retry():
                try:
                    async with semaphore:
                        if delay > 0:
                            await asyncio.sleep(delay)
                        await cache.fetch(id, refresh=True)
                        logger.debug(f"âœ… é è¼‰å…¥æˆåŠŸï¼š{id}")
                except Exception as e:
                    error_msg = f"é è¼‰å…¥å¤±æ•— {id}: {str(e)}"
                    logger.warning(f"âš ï¸ {error_msg}")
                    raise FetchError(error_msg)
            
            try:
                await _fetch_with_retry()
            except Exception as e:
                logger.error(f"âŒ {id} é‡è©¦è€—ç›¡ä»ç„¶å¤±æ•—: {str(e)}")

        # ä¸¦è¡Œè™•ç†æ‰€æœ‰ç›®æ¨™
        await asyncio.gather(*(process(t) for t in ids))
        logger.info("ğŸ é è¼‰å…¥å®Œæˆ")

    except Exception as e:
        error_msg = f"é è¼‰å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
        logger.error(f"âŒ {error_msg}")
        raise FetchError(error_msg)

if __name__ == "__main__":
    import asyncio
    asyncio.run(preload_all(only_missing=False, max_concurrent=10, delay=0.0))
