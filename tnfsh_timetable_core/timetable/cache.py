import re
from typing import Dict, Optional, List, Set
from datetime import datetime
import logging
import asyncio
from pathlib import Path
import json
from aiohttp import client_exceptions
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
    RetryError
)

from tnfsh_timetable_core.abc.cache_abc import BaseCacheABC
from tnfsh_timetable_core.index.models import TargetInfo
from tnfsh_timetable_core.timetable.models import (
    CacheMetadata,
    CachedTimeTable, 
    TimetableSchema
)
from tnfsh_timetable_core.index.index import Index
from tnfsh_timetable_core.timetable.crawler import TimetableCrawler, FetchError
from tnfsh_timetable_core.utils.logger import get_logger
from pydantic import BaseModel

logger = get_logger(logger_level="INFO")



# å…¨åŸŸè¨˜æ†¶é«”å¿«å–
_memory_cache: Dict[str, CachedTimeTable] = {}

class TimeTableCache(BaseCacheABC):
    """èª²è¡¨çš„å¿«å–å¯¦ä½œï¼Œæ”¯æ´ä¸‰å±¤å¿«å–æ¶æ§‹"""
    
    def __init__(
        self, 
        crawler: Optional[TimetableCrawler] = None,
        cache_dir: Optional[str] = None,
        file_path_template: str = "prebuilt_{target}_{id}.json",
        index: Optional[Index] = None,
        aliases: Optional[List[Set[str]]] = None
    ):
        """åˆå§‹åŒ–å¿«å–ç³»çµ±
        
        Args:
            crawler: èª²è¡¨çˆ¬èŸ²å¯¦ä¾‹ï¼Œå¦‚æœæœªæä¾›æœƒå»ºç«‹æ–°çš„å¯¦ä¾‹
            cache_dir: å¿«å–ç›®éŒ„è·¯å¾‘ï¼Œå¦‚æœæœªæä¾›å‰‡ä½¿ç”¨é è¨­è·¯å¾‘
            file_path_template: å¿«å–æª”æ¡ˆåç¨±æ¨¡æ¿ï¼Œå¯ç”¨ {target} åšæ›¿æ›
        """
        self._index = index
        self._crawler = crawler or TimetableCrawler(
            index=self._index,
            aliases=aliases,
        )
        self._cache_dir = Path(cache_dir) if cache_dir else Path(__file__).resolve().parent / "cache"
        self._file_path_template = file_path_template
        self._cache_dir.mkdir(exist_ok=True)

    def _get_cache_path(self, target: str, id: str) -> Path:
        """å–å¾—ç›®æ¨™çš„å¿«å–æª”æ¡ˆè·¯å¾‘ (target, id)"""
        safe_target = "".join(c for c in target if c.isalnum() or c in "-_")
        safe_id = "".join(c for c in id if c.isalnum() or c in "-_")
        return self._cache_dir / self._file_path_template.format(target=safe_target, id=safe_id)

    async def fetch_from_memory(self, id: str, target: str = "", *args, **kwargs) -> Optional[CachedTimeTable]:
        """ä»¥ id ç‚ºä¸»éµå¾å…¨åŸŸè®Šæ•¸å¿«å–å–å¾—èª²è¡¨è³‡æ–™ï¼Œlogger åŒæ™‚è¨˜éŒ„ target èˆ‡ id"""
        if id in _memory_cache:
            logger.debug(f"âœ¨ å¾è¨˜æ†¶é«”å¿«å–å–å¾—èª²è¡¨ï¼š{target} ({id})")
            return _memory_cache[id]
        return None

    async def save_to_memory(self, data: CachedTimeTable, id: str, target: str = "", *args, **kwargs) -> None:
        """ä»¥ id ç‚ºä¸»éµå„²å­˜èª²è¡¨è³‡æ–™åˆ°å…¨åŸŸè®Šæ•¸å¿«å–ï¼Œlogger åŒæ™‚è¨˜éŒ„ target èˆ‡ id"""
        _memory_cache[id] = data
        logger.debug(f"âœ¨ å·²æ›´æ–°è¨˜æ†¶é«”å¿«å–ï¼š{target} ({id})")

    async def fetch_from_file(self, target: str, id: str, *args, **kwargs) -> Optional[CachedTimeTable]:
        """ä»¥ id ç‚ºä¸»éµå¾æª”æ¡ˆå¿«å–å–å¾—èª²è¡¨è³‡æ–™ï¼Œlogger åŒæ™‚è¨˜éŒ„ target èˆ‡ id"""
        cache_path = self._get_cache_path(target, id)
        try:
            if not cache_path.exists():
                return None
            with open(cache_path, encoding="utf-8") as f:
                data = json.load(f)
                result = CachedTimeTable.model_validate(data)
                logger.debug(f"ğŸ’¾ å¾æª”æ¡ˆè¼‰å…¥èª²è¡¨å¿«å–ï¼š{target} ({id})")
                return result
        except Exception as e:
            logger.error(f"è®€å–å¿«å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e} [{target} ({id})]")
            return None

    async def save_to_file(self, data: CachedTimeTable, target: str, id: str, *args, **kwargs) -> None:
        """ä»¥ id ç‚ºä¸»éµå„²å­˜èª²è¡¨è³‡æ–™åˆ°æª”æ¡ˆå¿«å–ï¼Œlogger åŒæ™‚è¨˜éŒ„ target èˆ‡ id"""
        cache_path = self._get_cache_path(target, id)
        try:
            with open(cache_path, "w", encoding="utf-8") as f:
                json_data = data.model_dump_json(indent=4)
                f.write(json_data)
                logger.debug(f"ğŸ’¾ å·²æ›´æ–°æª”æ¡ˆå¿«å–ï¼š{target} ({id})")
        except Exception as e:
            logger.error(f"å„²å­˜å¿«å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e} [{target} ({id})]")
            raise FetchError(f"å„²å­˜å¿«å–æª”æ¡ˆå¤±æ•—: {str(e)} [{target} ({id})]")

    @retry(
        retry=retry_if_exception_type((
            FetchError,
            client_exceptions.ClientError,
            client_exceptions.ServerTimeoutError,
            asyncio.TimeoutError
        )),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=5),
        before_sleep=before_sleep_log(logger, logging.WARNING)
    )
    async def fetch_from_source(self, target: str, id: str, *args, **kwargs) -> CachedTimeTable:
        """å¾ç¶²è·¯ä¾†æºå–å¾—æœ€æ–°çš„èª²è¡¨è³‡æ–™ (target, id)ï¼Œlogger åŒæ™‚è¨˜éŒ„ target èˆ‡ id"""
        try:
            logger.info(f"ğŸŒ å¾ç¶²è·¯æŠ“å–èª²è¡¨ï¼š{target} ({id})")
            timetable = await self._crawler.fetch(target, refresh=kwargs.get('refresh', False))
            cached_result = CachedTimeTable(
                metadata=CacheMetadata(cache_fetch_at=datetime.now()),
                data=timetable
            )
            return cached_result
        except Exception as e:
            error_msg = f"å¾ä¾†æºæŠ“å–èª²è¡¨å¤±æ•— {target} ({id}): {str(e)}"
            logger.warning(f"âš ï¸ {error_msg}")
            raise FetchError(error_msg)

    async def fetch(
        self, 
        target: str, 
        *, 
        refresh: bool = False, 
        crawler: Optional[TimetableCrawler] = None,
        cache_dir: Optional[str] = None,
        file_path_template: str = "prebuilt_{target}_{id}.json",
        index: Optional[Index] = None,
        aliases: Optional[List[Set[str]]] = None,
        **kwargs) -> CachedTimeTable:
        """æ™ºèƒ½ç²å–èª²è¡¨è³‡æ–™ï¼Œä¸»éµç‚º idï¼Œæª”åç‚º prebuilt_target_id.jsonï¼Œlogger åŒæ™‚è¨˜éŒ„ target èˆ‡ id"""
        if not self._index:
            from tnfsh_timetable_core import TNFSHTimetableCore
            core = TNFSHTimetableCore()
            self._index = await core.fetch_index(refresh=refresh, aliases=aliases)
        target_info = self._index[target]
        if isinstance(target_info, TargetInfo):
            target_name = target_info.target
            id = target_info.id
        elif isinstance(target_info, list):
            raise ValueError(f"ç›®æ¨™ {target} æœ‰å¤šå€‹åˆ¥åï¼Œè«‹æŒ‡å®šå”¯ä¸€åˆ¥å: {', '.join(target_info)}")
        else:
            raise KeyError(f"ç›®æ¨™ {target} ä¸å­˜åœ¨æ–¼ç´¢å¼•ä¸­")
        if not refresh:
            mem = await self.fetch_from_memory(id, target_name)
            if mem:
                return mem
        if not refresh:
            file = await self.fetch_from_file(target_name, id)
            if file:
                await self.save_to_memory(file, id, target_name)
                return file
        net = await self.fetch_from_source(target_name, id)
        await self.save_to_file(net, target_name, id)
        await self.save_to_memory(net, id, target_name)
        return net

@retry(
    stop=stop_after_attempt(2),  # æ•´é«”æœ€å¤šé‡è©¦ 2 æ¬¡
    retry=retry_if_exception_type(FetchError),  # åªåœ¨ç²å–ç´¢å¼•å¤±æ•—æ™‚é‡è©¦
    wait=wait_exponential(multiplier=1, min=1, max=10),
    before_sleep=before_sleep_log(logger, logging.WARNING)
)
async def preload_all(only_missing: bool = True, max_concurrent: int = 5, delay: float = 0.0):
    """é è¼‰å…¥æ‰€æœ‰èª²è¡¨ï¼ŒåŠ å…¥ä½µç™¼ä¸Šé™èˆ‡å»¶é²æ§åˆ¶
    
    Args:
        only_missing: æ˜¯å¦åªè¼‰å…¥ç¼ºå°‘çš„èª²è¡¨ï¼Œé è¨­ç‚º True
        max_concurrent: æœ€å¤§ä½µç™¼è«‹æ±‚æ•¸é‡ï¼Œé è¨­ç‚º 5
        delay: æ¯ç­†è«‹æ±‚å‰çš„å»¶é²ç§’æ•¸ï¼Œé è¨­ç‚º 0
    """
    from tnfsh_timetable_core import TNFSHTimetableCore
    import asyncio

    try:
        # ç²å–ç´¢å¼•
        core = TNFSHTimetableCore()
        index = await core.fetch_index(refresh=True)
        
        
        if not index.reverse_index:
            error_msg = "âŒ ç„¡æ³•ç²å–èª²è¡¨ç´¢å¼•"
            logger.error(error_msg)
            raise FetchError(error_msg)

        targets = index.get_all_targets()
        logger.info(f"ğŸ”„ é–‹å§‹é è¼‰å…¥æ‰€æœ‰èª²è¡¨ï¼Œå…± {len(targets)} é …ï¼Œå»¶é²ï¼š{delay} ç§’ï¼Œä½µç™¼ä¸Šé™ï¼š{max_concurrent}")

        cache = TimeTableCache(index=index)
        semaphore = asyncio.Semaphore(max_concurrent)

        async def process(target: str):
            # æª¢æŸ¥æ˜¯å¦å·²æœ‰å¿«å–
            if only_missing:
                cached = await cache.fetch_from_memory(target) or await cache.fetch_from_file(target)
                if cached:
                    logger.debug(f"âš¡ å¿«å–å·²å­˜åœ¨ï¼Œç•¥éï¼š{target}")
                    return
                
            # å…§éƒ¨é‡è©¦å‡½æ•¸
            @retry(
                stop=stop_after_attempt(3),  # å–®å€‹ç›®æ¨™æœ€å¤šé‡è©¦ 3 æ¬¡
                retry=retry_if_exception_type((
                    FetchError,
                    client_exceptions.ClientError,
                    client_exceptions.ServerTimeoutError,
                    asyncio.TimeoutError
                )),
                wait=wait_exponential(multiplier=1, min=1, max=5),
                before_sleep=before_sleep_log(logger, logging.DEBUG)
            )
            async def _fetch_with_retry():
                try:
                    async with semaphore:
                        if delay > 0:
                            await asyncio.sleep(delay)
                        await cache.fetch(target, refresh=True)
                        logger.debug(f"âœ… é è¼‰å…¥æˆåŠŸï¼š{target}")
                except Exception as e:
                    error_msg = f"é è¼‰å…¥å¤±æ•— {target}: {str(e)}"
                    logger.warning(f"âš ï¸ {error_msg}")
                    raise FetchError(error_msg)
            
            try:
                await _fetch_with_retry()
            except Exception as e:
                logger.error(f"âŒ {target} é‡è©¦è€—ç›¡ä»ç„¶å¤±æ•—: {str(e)}")

        # ä¸¦è¡Œè™•ç†æ‰€æœ‰ç›®æ¨™
        await asyncio.gather(*(process(t) for t in targets))
        logger.info("ğŸ é è¼‰å…¥å®Œæˆ")

    except Exception as e:
        error_msg = f"é è¼‰å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
        logger.error(f"âŒ {error_msg}")
        raise FetchError(error_msg)

if __name__ == "__main__":
    import asyncio
    asyncio.run(preload_all(only_missing=False, max_concurrent=10, delay=0.0))
