from typing import List, Set, Dict, Optional, Literal, Tuple, TypedDict, TypeAlias
import logging
from unittest import result
import aiohttp
from aiohttp import client_exceptions
import asyncio
from bs4 import BeautifulSoup
import re
from pathlib import Path
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
    RetryError
)

from tnfsh_timetable_core.utils.logger import get_logger
from tnfsh_timetable_core.abc.crawler_abc import BaseCrawlerABC
from tnfsh_timetable_core.index.models import ReverseIndexResult

class FetchError(Exception):
    """çˆ¬å–èª²è¡¨æ™‚å¯èƒ½ç™¼ç”Ÿçš„éŒ¯èª¤"""
    def __init__(self, message: str):
        super().__init__(message)
        self.message = message

# è¨­å®šæ—¥èªŒ
logger = get_logger(logger_level="DEBUG")

TimeInfo: TypeAlias = Tuple[str, str]
PeriodName: TypeAlias = str
Subject: TypeAlias = str
CounterPart: TypeAlias = str
Url: TypeAlias = str
Course: TypeAlias = Dict[Subject, Dict[CounterPart, Url]]

class RawParsedResult(TypedDict):
    last_update: str
    periods: Dict[PeriodName, TimeInfo]
    table: List[List[Course]]

class TimetableCrawler(BaseCrawlerABC):
    """èª²è¡¨çˆ¬èŸ²å¯¦ä½œ"""
    
    # é è¨­åˆ¥ååˆ—è¡¨ï¼Œä½œç‚ºé¡åˆ¥å±¬æ€§
    DEFAULT_ALIASES: List[Set[str]] = [{"æœ±è’™", "å³éŠ˜"}]
    
    def __init__(self, aliases: Optional[List[Set[str]]] = None):
        """
        åˆå§‹åŒ–èª²è¡¨çˆ¬èŸ²

        Args:
            aliases (Optional[List[Set[str]]], optional): åˆ¥ååˆ—è¡¨. é è¨­ç‚º None
        """
        self.aliases = aliases or self.DEFAULT_ALIASES
        self._url_cache: Dict[str, str] = {}  # å¿«å–ä¸åŒç›®æ¨™çš„ URL

    @staticmethod
    def _parse_periods(row: BeautifulSoup) -> Optional[Tuple[str, Tuple[str, str]]]:
        """è§£æèª²è¡¨æ™‚é–“"""
        cells = row.find_all("td")
        if len(cells) < 2:
            return None
            
        lesson_name = cells[0].text.replace("\n", "").replace("\r", "")
        time_text = cells[1].text.replace("\n", "").replace("\r", "")
        
        re_pattern = r'(\d{2})(\d{2})'
        re_sub = r'\1:\2'
        times = [re.sub(re_pattern, re_sub, t.replace(" ", "")) for t in time_text.split("ï½œ")]
        if len(times) == 2:
            return (lesson_name, (times[0], times[1]))
        return None

    @staticmethod
    def _parse_cell(class_td: BeautifulSoup) -> Dict[str, Dict[str, str]]:
        """åˆ†æèª²ç¨‹tdå…ƒç´ ç‚ºèª²ç¨‹åç¨±å’Œæ•™å¸«åç¨±"""
        def clean_text(text: str) -> str:
            """æ¸…ç†æ–‡å­—å…§å®¹ï¼Œç§»é™¤å¤šé¤˜ç©ºæ ¼èˆ‡æ›è¡Œ"""
            return text.strip("\n").strip("\r").strip(" ").replace(" ", ", ")

        def is_teacher_p(p_tag: BeautifulSoup) -> bool:
            """æª¢æŸ¥æ˜¯å¦ç‚ºåŒ…å«æ•™å¸«è³‡è¨Šçš„pæ¨™ç±¤"""
            return bool(p_tag.find_all('a'))
        
        def parse_teachers(teacher_ps: List[BeautifulSoup]) -> Dict[str, str]:
            """è§£ææ‰€æœ‰æ•™å¸«pæ¨™ç±¤çš„è³‡è¨Š"""
            teachers_dict = {}
            for p in teacher_ps:
                for link in p.find_all('a'):
                    name = clean_text(link.text)
                    href = link.get('href', '')
                    teachers_dict[name] = href
            return teachers_dict
        
        def combine_class_name(class_ps: List[BeautifulSoup]) -> str:
            """çµ„åˆèª²ç¨‹åç¨±"""
            texts = [clean_text(p.text) for p in class_ps]
            return ''.join(filter(None, texts)).replace("\n", ", ").replace("\u00a0", "")
        
        ps = class_td.find_all('p')
        if not ps:
            return {"": {"": ""}}
        
        teacher_ps = []
        class_ps = []
        for p in ps:
            if is_teacher_p(p):
                teacher_ps.append(p)
            else:
                class_ps.append(p)
        
        teachers_dict = parse_teachers(teacher_ps) if teacher_ps else {"": ""}
        
        if class_ps:
            class_name = combine_class_name(class_ps)
        elif teacher_ps == {'':''}:
            class_name = "æ‰¾ä¸åˆ°èª²ç¨‹"
        else:
            class_name = ""
        
        if (class_name and class_name != " ") or teachers_dict != {"": ""}:
            return {class_name: teachers_dict}
        return {"": {"": ""}}
    
    def _resolve_target(self, target: str, reverse_index: ReverseIndexResult) -> Optional[str]:
        """æ ¹æ“šç›®æ¨™åç¨±è§£æåˆ¥å"""
        if target in reverse_index:
            logger.debug(f"ğŸ¯ æ‰¾åˆ° {target} çš„TimeTableç¶²å€")
            return target

        for alias_set in self.aliases:
            if target in alias_set:
                candidates = alias_set - {target}
                for alias in candidates:
                    if alias in reverse_index:
                        logger.info(f"ğŸ”„ å°‡ {target} è§£æç‚ºåˆ¥å {alias}")
                        return alias
                    logger.debug(f"æ‰¾ä¸åˆ° {alias} å°æ‡‰çš„TimeTableç¶²å€")
        return None

    async def _get_url(self, target: str, refresh: bool = False) -> str:
        """ç²å–ç›®æ¨™çš„å®Œæ•´ URL"""
        if not refresh and target in self._url_cache:
            return self._url_cache[target]
        from tnfsh_timetable_core import TNFSHTimetableCore
        core = TNFSHTimetableCore()
        index = await core.fetch_index(refresh=refresh)
        if index.reverse_index is None:
            logger.error("âŒ ç„¡æ³•ç²å–Indexè³‡æ–™")
            raise FetchError("ç„¡æ³•ç²å–Indexè³‡æ–™")

        logger.debug(f"ğŸ” è§£æç›®æ¨™ï¼š{target}")
        real_target = self._resolve_target(target, index.reverse_index)
        if real_target is None:
            logger.error(f"âŒ æ‰¾ä¸åˆ° {target} çš„Timetableç¶²å€")
            raise FetchError(f"æ‰¾ä¸åˆ° {target} çš„Timetableç¶²å€")

        if target == "307":
            relative_url = "C101307.html"
        else:
            relative_url = index.reverse_index[real_target]["url"]

        url = index.base_url + relative_url
        self._url_cache[target] = url
        logger.debug(f"ğŸŒ æº–å‚™è«‹æ±‚ç¶²å€ï¼š{url}")
        return url

    @retry(
        retry=retry_if_exception_type((
            client_exceptions.ClientError,
            client_exceptions.ServerTimeoutError,
            asyncio.TimeoutError
        )),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        before_sleep=before_sleep_log(logger, logging.WARNING),
        retry_error_cls=FetchError
    )
    async def fetch_raw(self, target: str, refresh: bool = False, *args, **kwargs) -> BeautifulSoup:
        """
        æŠ“å–åŸå§‹èª²è¡¨ HTML

        Args:
            target (str): ç›®æ¨™åç¨±ï¼ˆç­ç´šæˆ–æ•™å¸«ï¼‰
            refresh (bool, optional): æ˜¯å¦å¼·åˆ¶æ›´æ–°ç´¢å¼•å¿«å–. é è¨­ç‚º False

        Returns:
            BeautifulSoup: è§£æå¾Œçš„ HTML å†…å®¹

        Raises:
            FetchError: ç•¶ç™¼ç”Ÿç¶²è·¯è«‹æ±‚éŒ¯èª¤æ™‚æ‹‹å‡º
        """
        url = await self._get_url(target, refresh=refresh)
        headers = self.get_headers()

        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                logger.debug(f"ğŸ“¡ ç™¼é€è«‹æ±‚ï¼š{target}")
                async with session.get(url, headers=headers) as response:
                    response.raise_for_status()
                    content = await response.read()
                    logger.debug(f"ğŸ“¥ æ”¶åˆ°å›æ‡‰ï¼š{target}")
                    soup = BeautifulSoup(content, 'html.parser')
                    return soup

        except client_exceptions.ClientResponseError as e:
            error_msg = f"HTTP ç‹€æ…‹ç¢¼éŒ¯èª¤ {e.status}: {e.message}"
            logger.error(f"âŒ {error_msg}")
            raise FetchError(error_msg)

        except client_exceptions.ClientError as e:
            error_msg = f"ç¶²è·¯è«‹æ±‚éŒ¯èª¤ï¼š{str(e)}"
            logger.warning(f"âš ï¸ {error_msg}")
            raise  # è®“ tenacity è™•ç†é‡è©¦

        except (client_exceptions.ServerTimeoutError, asyncio.TimeoutError) as e:
            error_msg = "è«‹æ±‚è¶…æ™‚"
            logger.warning(f"âš ï¸ {error_msg}")
            raise  # è®“ tenacity è™•ç†é‡è©¦

        except Exception as e:
            error_msg = f"æœªé æœŸçš„éŒ¯èª¤ï¼š{str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise FetchError(error_msg)

    def parse(self, soup: BeautifulSoup, *args, **kwargs) -> RawParsedResult:
        """
        è§£æ BeautifulSoup ç‰©ä»¶ç‚ºçµæ§‹åŒ–è³‡æ–™

        Args:
            soup (BeautifulSoup): HTML è§£ææ¨¹

        Returns:
            RawParsedResult: è§£æå¾Œçš„çµæ§‹åŒ–è³‡æ–™

        Raises:
            FetchError: ç•¶è§£æå¤±æ•—æ™‚æ‹‹å‡º
        """
        
        try:
            # æ“·å–æ›´æ–°æ—¥æœŸ
            update_element = soup.find('p', class_='MsoNormal', align='center')
            if update_element:
                spans = update_element.find_all('span')
                last_update = spans[1].text if len(spans) > 1 else "No update date found."
                logger.debug(f"ğŸ“… æ›´æ–°æ—¥æœŸï¼š{last_update}")
            else:
                last_update = "No update date found."
                logger.warning("âš ï¸ æ‰¾ä¸åˆ°æ›´æ–°æ—¥æœŸ")

            # æ“·å–èª²è¡¨ table ä¸¦ç§»é™¤ border
            main_table = None
            for table in soup.find_all("table"):
                new_table = BeautifulSoup('<table></table>', 'html.parser').table
                for row in table.find_all("tr"):
                    for td in row.find_all('td'):
                        if td.get('style') and 'border' in td['style']:
                            td.decompose()
                    if len(row.find_all('td')) == 7:
                        new_table.append(row)
                        
                if len(new_table.find_all('tr')) > 0:
                    main_table = new_table
                    break

            if main_table is None:
                logger.error("âŒ æ‰¾ä¸åˆ°ç¬¦åˆæ ¼å¼çš„Timetable")
                raise FetchError("æ‰¾ä¸åˆ°ç¬¦åˆæ ¼å¼çš„Timetable")

            # æ“·å– periods
            periods: Dict[str, Tuple[str, str]] = {}
            for row in main_table.find_all("tr"):
                result = self._parse_periods(row)
                if result:
                    lesson_name, times = result
                    periods[lesson_name] = times

            # æ“·å– table raw æ ¼å¼
            table: List[List[Dict[str, Dict[str, str]]]] = []
            for row in main_table.find_all("tr"):
                cells = row.find_all("td")[2:]  # è·³éå‰å…©åˆ—ï¼ˆç¯€æ¬¡å’Œæ™‚é–“ï¼‰
                row_data = []
                for cell in cells:
                    row_data.append(self._parse_cell(cell))
                if row_data:
                    table.append(row_data)

            return RawParsedResult(
                last_update=last_update,
                periods=periods,
                table=table
            )
        except Exception as e:
            error_msg = f"è§£æéŒ¯èª¤ï¼š{str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise FetchError(error_msg)

    async def fetch(self, target: str, refresh: bool = False, *args, **kwargs) -> RawParsedResult:
        """
        å®Œæ•´çš„èª²è¡¨æŠ“å–æµç¨‹

        Args:
            target (str): ç›®æ¨™åç¨±ï¼ˆç­ç´šæˆ–æ•™å¸«ï¼‰
            refresh (bool, optional): æ˜¯å¦å¼·åˆ¶æ›´æ–°ç´¢å¼•å¿«å–. é è¨­ç‚º False

        Returns:
            RawParsedResult: è§£æå¾Œçš„èª²è¡¨è³‡æ–™

        Raises:
            FetchError: ç•¶æŠ“å–æˆ–è§£æå¤±æ•—æ™‚æ‹‹å‡º
        """
        raw_html = await self.fetch_raw(target, refresh=refresh)
        result = self.parse(raw_html)
        logger.info(f"âœ… {target}[æŠ“å–]å®Œæˆ")
        return result

if __name__ == "__main__":
    # For test cases, see: tests/test_timetable/test_crawler.py
    pass