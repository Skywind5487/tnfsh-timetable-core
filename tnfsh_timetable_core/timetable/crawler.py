from __future__ import annotations  
from operator import index
from typing import TYPE_CHECKING, List, Set, Dict, Optional, Literal, Tuple, TypedDict, TypeAlias
import logging
from unittest import result
import aiohttp
from aiohttp import client_exceptions
import asyncio
from bs4 import BeautifulSoup
import re
from pathlib import Path
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
    RetryError
)
if TYPE_CHECKING:
    from tnfsh_timetable_core.index.index import Index

from tnfsh_timetable_core.utils.logger import get_logger
from tnfsh_timetable_core.abc.crawler_abc import BaseCrawlerABC
from tnfsh_timetable_core.index.models import ReverseIndexResult, TargetInfo



# è¨­å®šæ—¥èªŒ
logger = get_logger(logger_level="INFO")

from tnfsh_timetable_core.timetable.models import (
    TimetableSchema,
    FetchError,
    CourseInfo,
    CounterPart
)
from tnfsh_timetable_core.index.index import Index

class TimetableCrawler(BaseCrawlerABC):
    """èª²è¡¨çˆ¬èŸ²å¯¦ä½œ"""
    
    # é è¨­åˆ¥ååˆ—è¡¨ï¼Œä½œç‚ºé¡åˆ¥å±¬æ€§
    DEFAULT_ALIASES: List[Set[str]] = [{"æœ±è’™", "å³éŠ˜"}]
    
    def __init__(self, 
                 aliases: Optional[List[Set[str]]] = None,
                 index: Optional[Index] = None
    ):
        """
        åˆå§‹åŒ–èª²è¡¨çˆ¬èŸ²

        Args:
            aliases (Optional[List[Set[str]]], optional): åˆ¥ååˆ—è¡¨. é è¨­ç‚º None
        """
        self.aliases = aliases or self.DEFAULT_ALIASES
        self._url_cache: Dict[str, str] = {}  # å¿«å–ä¸åŒç›®æ¨™çš„ URL
        self._index : Index | None = index  # ç”¨æ–¼å­˜å„²ç´¢å¼•è³‡æ–™

    @staticmethod
    def _parse_periods(row: BeautifulSoup) -> Optional[Tuple[str, Tuple[str, str]]]:
        """è§£æèª²è¡¨æ™‚é–“"""
        cells = row.find_all("td")
        if len(cells) < 2:
            return None
            
        lesson_name = cells[0].text.replace("\n", "").replace("\r", "")
        time_text = cells[1].text.replace("\n", "").replace("\r", "")
        
        re_pattern = r'(\d{2})(\d{2})'
        re_sub = r'\1:\2'
        times = [re.sub(re_pattern, re_sub, t.replace(" ", "")) for t in time_text.split("ï½œ")]
        if len(times) == 2:
            return (lesson_name, (times[0], times[1]))
        return None 
    
    @staticmethod
    def _parse_cell(class_td: BeautifulSoup) -> Optional[CourseInfo]:
        """åˆ†æèª²ç¨‹tdå…ƒç´ ç‚º CourseInfo æ ¼å¼"""
        def clean_text(text: str) -> str:
            """æ¸…ç†æ–‡å­—å…§å®¹ï¼Œç§»é™¤å¤šé¤˜ç©ºæ ¼èˆ‡æ›è¡Œ"""
            return text.replace("\n", "").replace("\r", "").strip(" ").replace(" ", "")

        def is_teacher_p(p_tag: BeautifulSoup) -> bool:
            """æª¢æŸ¥æ˜¯å¦ç‚ºåŒ…å«æ•™å¸«è³‡è¨Šçš„pæ¨™ç±¤"""
            return bool(p_tag.find_all('a'))
        
        def parse_teachers(teacher_ps: List[BeautifulSoup]) -> Dict[str, str]:
            """è§£ææ‰€æœ‰æ•™å¸«pæ¨™ç±¤çš„è³‡è¨Š"""
            teachers_dict = {}
            for p in teacher_ps:
                for link in p.find_all('a'):
                    name = clean_text(link.text)
                    href = link.get('href', '')
                    teachers_dict[name] = href
            return teachers_dict
        
        def combine_class_name(class_ps: List[BeautifulSoup]) -> str:
            """çµ„åˆèª²ç¨‹åç¨±"""
            texts = [clean_text(p.text) for p in class_ps]
            return ''.join(filter(None, texts)).replace("\n", ", ").replace("\u00a0", "")
        ps = class_td.find_all('p')
        if not ps:
            return None
        
        teacher_ps = []
        class_ps = []
        for p in ps:
            if is_teacher_p(p):
                teacher_ps.append(p)
            else:
                class_ps.append(p)
        
        # è§£ææ•™å¸«è³‡è¨Šç‚º CounterPart åˆ—è¡¨
        counterparts = []
        if teacher_ps:
            for p in teacher_ps:
                for link in p.find_all('a'):
                    name = clean_text(link.text)
                    href = link.get('href', '')
                    if name and href:  # åªæœ‰ç•¶åå­—å’Œé€£çµéƒ½å­˜åœ¨æ™‚æ‰åŠ å…¥
                        counterparts.append(CounterPart(participant=name, url=href))
        
        # è§£æèª²ç¨‹åç¨±
        subject = ""
        if class_ps:
            subject = combine_class_name(class_ps)
        elif not counterparts:  # å¦‚æœæ—¢æ²’æœ‰èª²ç¨‹åç¨±ä¹Ÿæ²’æœ‰æ•™å¸«
            return None
        
        # è¿”å› CourseInfo ç‰©ä»¶
        if subject or counterparts:
            return CourseInfo(
                subject=subject,
                counterpart=counterparts if counterparts else None
            )
        return None
    
    def _resolve_target(self, target: str, index: Index) -> Optional[TargetInfo]:
        """æ ¹æ“šç›®æ¨™åç¨±è§£æä¸¦è¿”å› TargetInfo æˆ– URL"""
        result = index[target]

        if result:
            if isinstance(result, list):
                raise KeyError(f"ğŸ”„ {target} æœ‰å¤šå€‹å°æ‡‰çš„ID: {result}")
            else:
                logger.debug(f"ğŸ¯ æ‰¾åˆ° {target} çš„TimeTableç¶²å€")
                return result

        for alias_set in self.aliases:
            if target in alias_set:
                candidates = alias_set - {target}
                for alias in candidates:
                    tmp_result = index[alias]
                    if tmp_result:
                        if isinstance(tmp_result, list):
                            raise KeyError(f"ğŸ”„ {alias} æœ‰å¤šå€‹å°æ‡‰çš„ID: {tmp_result}")
                        else:
                            logger.info(f"ğŸ”„ å°‡ {target} è§£æç‚ºåˆ¥å {alias}")
                            return tmp_result
                    logger.debug(f"æ‰¾ä¸åˆ° {alias} å°æ‡‰çš„TimeTableç¶²å€")
        return None

    async def _resolve_target_info(self, target: str, refresh: bool = False) -> tuple[TargetInfo, str]:
        """
        è§£æç›®æ¨™åç¨±ä¸¦å–å¾— TargetInfo èˆ‡å®Œæ•´ URL
        """
        if not self._index:
            # å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œå‰‡é‡æ–°æŠ“å–ç´¢å¼•
            from tnfsh_timetable_core.index.index import Index
            self._index = await Index.fetch(refresh=refresh)
        index = self._index
        real_target = self._resolve_target(target, index)
        if real_target is None:
            logger.error(f"âŒ æ‰¾ä¸åˆ° {target} çš„Timetableç¶²å€")
            raise FetchError(f"æ‰¾ä¸åˆ° {target} çš„Timetableç¶²å€")
        relative_url = real_target.url
        url = index.base_url + relative_url
        self._url_cache[target] = url  # åƒ…å¿«å– url
        logger.debug(f"ğŸŒ æº–å‚™è«‹æ±‚ç¶²å€ï¼š{url}")
        return real_target, url

    @retry(
        retry=retry_if_exception_type((
            client_exceptions.ClientError,
            client_exceptions.ServerTimeoutError,
            asyncio.TimeoutError
        )),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        before_sleep=before_sleep_log(logger, logging.WARNING),
        retry_error_cls=FetchError
    )
    async def fetch_raw(self, target: str, refresh: bool = False, *args, **kwargs) -> tuple[BeautifulSoup, TargetInfo, str]:
        """
        æŠ“å–åŸå§‹èª²è¡¨ HTMLï¼Œä¸¦å›å‚³ TargetInfo èˆ‡ url
        """
        target_info, url = await self._resolve_target_info(target, refresh=refresh)
        headers = self.get_headers()
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                logger.debug(f"ğŸ“¡ ç™¼é€è«‹æ±‚ï¼š{target}")
                async with session.get(url, headers=headers) as response:
                    response.raise_for_status()
                    content = await response.read()
                    logger.debug(f"ğŸ“¥ æ”¶åˆ°å›æ‡‰ï¼š{target}")
                    soup = BeautifulSoup(content, 'html.parser')
                    return soup, target_info, url
        except client_exceptions.ClientResponseError as e:
            error_msg = f"HTTP ç‹€æ…‹ç¢¼éŒ¯èª¤ {e.status}: {e.message}"
            logger.error(f"âŒ {error_msg}")
            raise FetchError(error_msg)
        except client_exceptions.ClientError as e:
            error_msg = f"ç¶²è·¯è«‹æ±‚éŒ¯èª¤ï¼š{str(e)}"
            logger.warning(f"âš ï¸ {error_msg}")
            raise  # è®“ tenacity è™•ç†é‡è©¦
        except (client_exceptions.ServerTimeoutError, asyncio.TimeoutError) as e:
            error_msg = "è«‹æ±‚è¶…æ™‚"
            logger.warning(f"âš ï¸ {error_msg}")
            raise  # è®“ tenacity è™•ç†é‡è©¦
        except Exception as e:
            error_msg = f"æœªé æœŸçš„éŒ¯èª¤ï¼š{str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise FetchError(error_msg)

    def parse(self, soup: BeautifulSoup, target_info: TargetInfo, target_url: str, *args, **kwargs) -> TimetableSchema:
        """
        è§£æ BeautifulSoup ç‰©ä»¶ç‚ºçµæ§‹åŒ–è³‡æ–™ï¼Œæ”¯æ´åˆä¼‘èª²ç¨‹åˆ†é›¢ã€‚

        Args:
            soup (BeautifulSoup): HTML è§£ææ¨¹
            target_info (TargetInfo): ç›®æ¨™è³‡è¨Šï¼ˆå« id, role, categoryï¼‰
            target_url (str): ç›®æ¨™çš„èª²è¡¨é€£çµ

        Returns:
            TimetableSchema: è§£æå¾Œçš„çµæ§‹åŒ–è³‡æ–™

        Raises:
            FetchError: ç•¶è§£æå¤±æ•—æ™‚æ‹‹å‡º
        """
        try:
            # æ“·å–æ›´æ–°æ—¥æœŸ
            update_element = soup.find('p', class_='MsoNormal', align='center')
            if update_element:
                span = update_element.find('span').find('span')
                last_update = span.text if span else "No update date found."
                logger.debug(f"ğŸ“… æ›´æ–°æ—¥æœŸï¼š{last_update}")
            else:
                last_update = "No update date found."
                logger.warning("âš ï¸ æ‰¾ä¸åˆ°æ›´æ–°æ—¥æœŸ")

            # æ“·å–èª²è¡¨ table ä¸¦ç§»é™¤ border
            main_table = None
            for table in soup.find_all("table"):
                new_table = BeautifulSoup('<table></table>', 'html.parser').table
                for row in table.find_all("tr"):
                    for td in row.find_all('td'):
                        if td.get('style') and 'border' in td['style']:
                            td.decompose()
                    if len(row.find_all('td')) == 7:
                        new_table.append(row)
                if len(new_table.find_all('tr')) > 0:
                    main_table = new_table
                    break

            if main_table is None:
                logger.error("âŒ æ‰¾ä¸åˆ°ç¬¦åˆæ ¼å¼çš„Timetable")
                raise FetchError("æ‰¾ä¸åˆ°ç¬¦åˆæ ¼å¼çš„Timetable")

            # æ“·å– periodsï¼Œä¸¦åµæ¸¬åˆä¼‘
            periods: Dict[str, Tuple[str, str]] = {}
            lunch_break_periods: Dict[str, Tuple[str, str]] = {}
            lunch_break_col = None
            current_lesson_count:int = 0
            for row in main_table.find_all("tr"):
                result = self._parse_periods(row)
                if result:
                    current_lesson_count += 1
                    lesson_name, times = result
                    # åˆä¼‘é—œéµå­—åµæ¸¬
                    if "åˆä¼‘" in lesson_name:
                        lunch_break_col = current_lesson_count - 1  # åˆä¼‘èª²ç¨‹æ‰€åœ¨çš„åˆ—
                        lunch_break_periods[lesson_name] = times
                    else:
                        periods[lesson_name] = times

            if not lunch_break_periods:
                lunch_break_periods = None

            # æ“·å– table raw æ ¼å¼ï¼Œä¸¦åˆ†é›¢åˆä¼‘èª²ç¨‹
            from tnfsh_timetable_core.timetable.models import CourseInfo
            table: List[List[CourseInfo | None]] = []
            lunch_break: List[CourseInfo | None] = []

            for i, row in enumerate(main_table.find_all("tr")):
                cells = row.find_all("td")[2:]  # è·³éå‰å…©åˆ—ï¼ˆç¯€æ¬¡å’Œæ™‚é–“ï¼‰
                row_data = []
                for j, cell in enumerate(cells):
                    course = self._parse_cell(cell)
                    logger.debug(f"lunch_break_col: {lunch_break_col}, i: {i}, j: {j}")
                    # è‹¥æœ¬è¡Œç‚ºåˆä¼‘ç¯€æ¬¡ï¼Œåˆ†é›¢å­˜å…¥ lunch_break
                    if lunch_break_col is not None and i == lunch_break_col:
                        lunch_break.append(course)
                    else:
                        row_data.append(course)
                if row_data:
                    table.append(row_data)
            # è¡Œåˆ—äº’æ›
            table = list(map(list, zip(*table)))

            
            if len(lunch_break) == 0:
                lunch_break = None
        except Exception as e:
            error_msg = f"è§£æéŒ¯èª¤ï¼š{str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise FetchError(error_msg)
        # return æ‹¿åˆ° try å€å¡Šå¤–ï¼Œé¿å… except æ””æˆª model é©—è­‰éŒ¯èª¤
        return TimetableSchema(
            # TargetInfo ç›¸é—œè³‡è¨Š
            target=target_info.target,
            category=target_info.category,
            target_url=target_info.url,
            role=target_info.role,
            id=target_info.id,
            # å…¶ä»–è³‡è¨Š
            last_update=last_update,
            # èª²è¡¨æ ¸å¿ƒè³‡æ–™
            table=table,
            periods=periods,
            lunch_break=lunch_break,
            lunch_break_periods=lunch_break_periods,
        )

    async def fetch(self, target: str, refresh: bool = False, *args, **kwargs) -> TimetableSchema:
        """
        å®Œæ•´çš„èª²è¡¨æŠ“å–æµç¨‹ï¼ŒTargetInfo å…¨ç¨‹è²«ç©¿
        """
        soup, target_info, url = await self.fetch_raw(target, refresh=refresh)
        result = self.parse(soup, target_info=target_info, target_url=url)
        logger.info(f"âœ… {target}({target_info.id})[æŠ“å–]å®Œæˆ")
        return result

if __name__ == "__main__":
    import asyncio
    async def main():
        crawler = TimetableCrawler()
        target = "é™³æšæ·"  # æ›¿æ›ç‚ºå¯¦éš›çš„ç­ç´šæˆ–æ•™å¸«åç¨±
        timetable = await crawler.fetch(target, refresh=True)
        with open(f"{target}_timetable.json", "w", encoding="utf-8") as f:
            f.write(timetable.model_dump_json(indent=4))

        from tnfsh_timetable_core.index.index import Index
        index = await Index.fetch(refresh=False)
        index["C101205.HTML"]

    asyncio.run(main())