from __future__ import annotations
from crawler import RawParsedResult
from typing import List, Dict, TypeAlias, Optional, Any, Literal
from pydantic import BaseModel
from datetime import datetime
from typing import ClassVar
import json
from tnfsh_class_table.utils.logger import get_logger

# è¨­å®šæ—¥èªŒ
logger = get_logger(logger_level="INFO")


class ScheduleEntry(BaseModel):
    weekday: int   # 1â€“5 (Monâ€“Fri)
    period: int    # 1â€“8
    subject: str
    teacher: str
    class_code: str


TimeSlot: TypeAlias = tuple[int, int]  # (weekday, period)

class Lookup(BaseModel):
    """
    ç”¨å…©å±¤ dict å–ä»£è¤‡é›œ SQLï¼š
    - teacher_lookup[teacher][(weekday, period)] = ScheduleEntry
    - class_lookup[class_code][(weekday, period)] = ScheduleEntry
    """
    teacher_lookup: Dict[str, Dict[TimeSlot, ScheduleEntry]]
    class_lookup:  Dict[str, Dict[TimeSlot, ScheduleEntry]]
    last_update:   datetime  # åš´æ ¼æ ¼å¼ï¼š%Y/%m/%d %H:%M:%S

class CounterPart(BaseModel):
    participant: str
    url: str

class CourseInfo(BaseModel):
    subject: str
    main: str # name of the class or teacher
    counterpart: Optional[List[CounterPart]] # name of the class or teacher


class ClassTable(BaseModel):
    table: List[List[Optional[CourseInfo]]]  # 5 weekdays x 8 periods
    type: Literal["class", "teacher"]
    target: str
    target_url: str

    @classmethod
    def from_parsed(cls, target: str, parsed: RawParsedResult) -> "ClassTable":
        from tnfsh_class_table.backend import TNFSHClassTableIndex

        reverse_index = TNFSHClassTableIndex.get_instance().reverse_index
        target_url = reverse_index[target]["url"]
        type_ = "class" if target.isdigit() else "teacher"

        # âœ… æ–°å¢ï¼šè½‰ç½® parsed["table"] â†’ weekday-major
        raw_table = parsed["table"]  # shape: [period][weekday]
        rotated_table = list(map(list, zip(*raw_table)))  # shape: [weekday][period]

        table: List[List[Optional[CourseInfo]]] = []

        for row in rotated_table:  # âœ… æ”¹æˆè™•ç†è½‰ç½®å¾Œçš„ row
            parsed_row: List[Optional[CourseInfo]] = []
            for cell in row:
                if not cell or cell == {"": {"": ""}}:
                    parsed_row.append(None)
                    continue

                subject = next(iter(cell))
                teachers_or_classes = cell[subject]  # Dict[name, url]

                # Prepare counterpart list
                counterpart_list = [
                    CounterPart(participant=name, url=url)
                    for name, url in teachers_or_classes.items()
                    if url and url != target_url
                ]

                # åˆ¤æ–·ä¸»é«”åç¨±
                main_name = target
                for name, url in teachers_or_classes.items():
                    if url == target_url:
                        main_name = name
                        break

                parsed_row.append(CourseInfo(
                    subject=subject,
                    main=main_name,
                    counterpart=counterpart_list if counterpart_list else None
                ))
            table.append(parsed_row)

        return cls(
            table=table,
            type=type_,
            target=target,
            target_url=target_url
        )

    @classmethod
    async def fetch_cached(cls, target: str, refresh: bool = False) -> "ClassTable":
        """
        æ”¯æ´ä¸‰å±¤å¿«å–çš„æ™ºèƒ½è¼‰å…¥æ–¹æ³•ï¼š
        1. è¨˜æ†¶é«” â†’ 2. æœ¬åœ°æª”æ¡ˆ â†’ 3. ç¶²è·¯è«‹æ±‚ï¼ˆå¯é€é refresh å¼·åˆ¶é‡æ–°å»ºç«‹ï¼‰
        ä¸¦åœ¨ refresh æ™‚åŒæ­¥æ›´æ–°è¨˜æ†¶é«”èˆ‡æœ¬åœ°å¿«å–ã€‚
        """
        from tnfsh_class_table.new_backend.cache import prebuilt_cache, load_from_disk, save_to_disk

        key = target

        # å±¤ 1ï¼šè¨˜æ†¶é«”
        if not refresh and key in prebuilt_cache:
            logger.debug(f"âœ¨ å¾è¨˜æ†¶é«”å¿«å–å–å¾—èª²è¡¨ï¼š{target}")
            return prebuilt_cache[key]

        # å±¤ 2ï¼šæœ¬åœ° JSON
        if not refresh:
            logger.debug(f"ğŸ’¾ å˜—è©¦å¾æœ¬åœ°å¿«å–è¼‰å…¥ï¼š{target}")
            data = load_from_disk(target)
            if data:
                try:
                    # å˜—è©¦å¾ JSON è¼‰å…¥è³‡æ–™
                    instance = cls.model_validate(data)
                    prebuilt_cache[key] = instance
                    logger.debug(f"ğŸ“¥ æˆåŠŸå¾æœ¬åœ°å¿«å–è¼‰å…¥ï¼š{target}")
                    return instance
                except Exception as e:
                    logger.error(f"âŒ æœ¬åœ°å¿«å–è³‡æ–™ç„¡æ•ˆï¼š{target}ï¼ŒéŒ¯èª¤ï¼š{e}")

        # å±¤ 3ï¼šfallback â†’ ç¶²è·¯ request
        logger.info(f"ğŸŒ å¾ç¶²è·¯æŠ“å–èª²è¡¨è³‡æ–™ï¼š{target}")
        instance = await cls._request(target)

        # åŒæ­¥æ›´æ–°å…©å±¤ cache
        prebuilt_cache[key] = instance
        save_to_disk(target, instance)
        logger.debug(f"ğŸ’¾ å·²æ›´æ–°å¿«å–ï¼š{target}")

        return instance

    @classmethod
    async def _request(cls, target: str) -> "ClassTable":
        """å¾ç¶²è·¯æŠ“å–èª²è¡¨è³‡æ–™ã€‚"""
        from crawler import fetch_raw_html, parse_html
        try:
            logger.debug(f"ğŸ“¡ æ­£åœ¨æŠ“å–èª²è¡¨é é¢ï¼š{target}")
            soup = await fetch_raw_html(target)
            logger.debug(f"ğŸ” è§£æèª²è¡¨è³‡æ–™ï¼š{target}")
            parsed = parse_html(soup)
            logger.debug(f"âœ… èª²è¡¨è³‡æ–™è§£æå®Œæˆï¼š{target}")
            return cls.from_parsed(target, parsed)
        except Exception as e:
            logger.error(f"âŒ æŠ“å–èª²è¡¨å¤±æ•—ï¼š{target}ï¼ŒéŒ¯èª¤ï¼š{e}")
            raise

if __name__ == "__main__":
    # æ¸¬è©¦ç”¨
    import asyncio
    from tnfsh_class_table.backend import TNFSHClassTableIndex

    async def main():
        logger.info("ğŸš€ é–‹å§‹æ¸¬è©¦èª²è¡¨è¼‰å…¥")
        table = await ClassTable.fetch_cached("317", refresh=True)
        logger.info("âœ¨ æ¸¬è©¦å®Œæˆï¼Œè¼¸å‡ºèª²è¡¨è³‡æ–™")
        #print(table.model_dump_json(indent=4))

    asyncio.run(main())