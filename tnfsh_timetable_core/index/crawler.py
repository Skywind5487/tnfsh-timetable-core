from typing import Optional, TypeAlias, Dict, Union
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import re
from pydantic import BaseModel

URL: TypeAlias = str
ItemMap: TypeAlias = Dict[str, URL]  # e.g. {"é»ƒå¤§å€¬": "TA01.html"} æˆ– {"101": "C101101.html"}
CategoryName: TypeAlias = str
CategoryMap: TypeAlias = Dict[CategoryName, ItemMap]  # e.g. {"åœ‹æ–‡ç§‘": {...}}, {"é«˜ä¸€": {...}}

# ========================
# ğŸ“¦ è³‡æ–™çµæ§‹æ¨¡å‹
# ========================

class GroupIndex(BaseModel):
    """
    è¡¨ç¤ºä¸€å€‹é¡åˆ¥çš„ç´¢å¼•è³‡æ–™ï¼Œä¾‹å¦‚ç­ç´šã€è€å¸«ç­‰ã€‚
    åŒ…å«ä¸€å€‹ URL èˆ‡ä¸€å±¤å·¢ç‹€å­—å…¸çµæ§‹çš„è³‡æ–™ã€‚
    """
    url: URL
    data: CategoryMap

    def __getitem__(self, key: str) -> ItemMap:
        return self.data[key]


class IndexResult(BaseModel):
    """
    è¡¨ç¤º index å€å¡Šçš„ä¸»çµæ§‹ï¼Œå«æœ‰ base_urlã€rootï¼Œä»¥åŠç­ç´šèˆ‡è€å¸«çš„ç´¢å¼•è³‡æ–™ã€‚
    """
    base_url: URL
    root: str
    class_: GroupIndex
    teacher: GroupIndex

async def fetch_html(base_url: str, url: str, timeout: int = 10, from_file_path: Optional[str] = None) -> BeautifulSoup:
    """éåŒæ­¥å–å¾—ç¶²é å…§å®¹ä¸¦è§£æ
    
    Args:
        base_url (str): åŸºç¤ URL
        url (str): ç›¸å°è·¯å¾‘ URL
        timeout (int): è«‹æ±‚è¶…æ™‚æ™‚é–“
        from_file_path (Optional[str]): å¯é¸çš„æª”æ¡ˆè·¯å¾‘ï¼Œè‹¥æä¾›å‰‡å¾è©²æª”æ¡ˆè®€å–
        
    Returns:
        BeautifulSoup: è§£æå¾Œçš„ BeautifulSoup ç‰©ä»¶
        
    Raises:
        aiohttp.ClientError: ç•¶ç¶²é è«‹æ±‚å¤±æ•—æ™‚
        Exception: ç•¶è§£æ HTML å¤±æ•—æ™‚
    """
    if from_file_path:
        with open(from_file_path, 'r', encoding='utf-8') as f:
            return BeautifulSoup(f.read(), 'html.parser')
            
    async with aiohttp.ClientSession() as session:
        async with session.get(base_url + url, timeout=timeout) as response:
            response.raise_for_status()
            content = await response.read()
            return BeautifulSoup(content, 'html.parser')

def parse_html(soup: BeautifulSoup, url: str) -> GroupIndex:
    """è§£æç¶²é å…§å®¹
    
    Args:
        soup (BeautifulSoup): è¦è§£æçš„ BeautifulSoup ç‰©ä»¶
        url (str): è©²ç´¢å¼•çš„ URL

    Returns:
        GroupIndex: è§£æå¾Œçš„ç´¢å¼•è³‡æ–™çµæ§‹
    """
    parsed_data = {}
    current_category = None
    
    for tr in soup.find_all("tr"):
        category_tag = tr.find("span")
        if category_tag and not tr.find("a"):
            current_category = category_tag.text.strip()
            parsed_data[current_category] = {}
        for a in tr.find_all("a"):
            link = a.get("href")
            text = a.text.strip()
            if text.isdigit() and link:
                parsed_data[current_category][text] = link
            else:
                match = re.search(r'([\u4e00-\u9fa5]+)', text)
                if match:
                    text = match.group(1)
                    parsed_data[current_category][text] = link
                else:
                    text = text.replace("\r", "").replace("\n", "").replace(" ", "").strip()
                    if len(text) > 3:
                        text = text[3:].strip()
                        parsed_data[current_category][text] = link
    
    return GroupIndex(url=url, data=parsed_data)

async def fetch_index(base_url: str) -> IndexResult:
    """éåŒæ­¥ç²å–å®Œæ•´çš„èª²è¡¨ç´¢å¼•
    
    Args:
        base_url (str): åŸºç¤ URL
        
    Returns:
        IndexResult: å®Œæ•´çš„èª²è¡¨ç´¢å¼•è³‡æ–™
    """
    # ä¸¦è¡Œç²å–æ•™å¸«å’Œç­ç´šç´¢å¼•
    tasks = [
        fetch_html(base_url, "_TeachIndex.html"),
        fetch_html(base_url, "_ClassIndex.html")
    ]
    teacher_soup, class_soup = await asyncio.gather(*tasks)
    
    # è§£æè³‡æ–™
    teacher_result = parse_html(teacher_soup, "_TeachIndex.html")
    class_result = parse_html(class_soup, "_ClassIndex.html")
    
    # å»ºç«‹å®Œæ•´ç´¢å¼•
    return IndexResult(
        base_url=base_url,
        root="index.html",
        class_=class_result,
        teacher=teacher_result
    )

# æ›´æ–°ä¸»ç¨‹å¼ç‚ºéåŒæ­¥ç‰ˆæœ¬
if __name__ == "__main__":
    # http://w3.tnfsh.tn.edu.tw/deanofstudies/course/_TeachIndex.html
    base_url = "http://w3.tnfsh.tn.edu.tw/deanofstudies/course/"
    
    async def main():
        index_result = await fetch_index(base_url)
        with open("index.json", "w", encoding="utf-8") as f:
            f.write(index_result.model_dump_json(indent=4))
    
    # åœ¨ Windows ä¸Šé‹è¡ŒéåŒæ­¥ç¨‹å¼
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
