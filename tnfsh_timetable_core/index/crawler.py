from typing import Optional, Dict, Tuple
from unittest.mock import DEFAULT
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import re
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
)
import logging

from tnfsh_timetable_core.abc.crawler_abc import BaseCrawlerABC
from tnfsh_timetable_core.index.models import IndexResult, GroupIndex
from tnfsh_timetable_core import TNFSHTimetableCore

core = TNFSHTimetableCore()
logger = core.get_logger(logger_level="DEBUG")

class IndexCrawler(BaseCrawlerABC):
    """é¦–é ç´¢å¼•çˆ¬èŸ²ï¼Œè² è²¬ç²å–èª²è¡¨é¦–é çš„ç´¢å¼•è³‡è¨Š"""

    DEFAULT_BASE_URL = "http://w3.tnfsh.tn.edu.tw/deanofstudies/course"
    DEFAULT_TEACHER_PAGE = "_TeachIndex.html"
    DEFAULT_CLASS_PAGE = "_ClassIndex.html"
    DEFAULT_ROOT = "index.html"

    def __init__(self, 
                 base_url: Optional[str] = None,
                 root_page: Optional[str] = None,
                 teacher_page: Optional[str] = None,
                 class_page: Optional[str] = None
                 ):
        """
        åˆå§‹åŒ–çˆ¬èŸ²
        
        Args:
            base_url: åŸºç¤ URLï¼Œå¦‚æœæœªæŒ‡å®šå‰‡ä½¿ç”¨é è¨­å€¼
        """
        self.base_url = base_url or self.DEFAULT_BASE_URL
        self.root = root_page or self.DEFAULT_ROOT
        self.teacher_page = teacher_page or self.DEFAULT_TEACHER_PAGE
        self.class_page = class_page or self.DEFAULT_CLASS_PAGE
        

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((
            aiohttp.ClientError,
            aiohttp.ServerTimeoutError,
            asyncio.TimeoutError
        )),
        before_sleep=before_sleep_log(logger, logging.WARNING)
    )
    async def fetch_raw(
        self,
        url: str,
        *,
        from_file: Optional[str] = None,
        timeout: int = 15
    ) -> BeautifulSoup:
        """ç²å–åŸå§‹ HTML å…§å®¹
        
        Args:
            url: ç›¸å° URL è·¯å¾‘
            from_file: å¯é¸çš„æœ¬åœ°æ–‡ä»¶è·¯å¾‘
            timeout: è«‹æ±‚è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            BeautifulSoup: è§£æå¾Œçš„ HTML
            
        Raises:
            aiohttp.ClientError: ç•¶ç™¼ç”Ÿç¶²è·¯éŒ¯èª¤æ™‚
            asyncio.TimeoutError: ç•¶è«‹æ±‚è¶…æ™‚æ™‚
        """
        if from_file:
            logger.debug(f"ğŸ“‚ å¾æª”æ¡ˆè®€å–ï¼š{from_file}")
            with open(from_file, 'r', encoding='utf-8') as f:
                return BeautifulSoup(f.read(), 'html.parser')
        
        logger.debug(f"ğŸŒ è«‹æ±‚ç¶²å€ï¼š{url}")
        
        async with aiohttp.ClientSession() as session:
            async with session.get(
                url,
                headers=self.get_headers(),
                timeout=aiohttp.ClientTimeout(total=timeout)
            ) as response:
                response.raise_for_status()
                content = await response.read()
                logger.debug(f"ğŸ“¥ æ”¶åˆ°å›æ‡‰ï¼š{len(content)} bytes")
                return BeautifulSoup(content, 'html.parser')

    def parse(self, raw: BeautifulSoup, url: str) -> GroupIndex:
        """è§£æ HTML å…§å®¹ç‚ºç´¢å¼•çµæ§‹
        
        Args:
            raw: BeautifulSoup ç‰©ä»¶
            url: ç´¢å¼•çš„ç›¸å° URL
            
        Returns:
            GroupIndex: è§£æå¾Œçš„ç´¢å¼•è³‡æ–™
        """
        parsed_data: Dict[str, Dict[str, str]] = {}
        current_category = None
        
        for tr in raw.find_all("tr"):
            # è™•ç†åˆ†é¡æ¨™é¡Œ
            if self._is_category_row(tr):
                current_category = tr.find("span").text.strip()
                parsed_data[current_category] = {}
                continue
            
            # è™•ç†åˆ†é¡å…§å®¹
            if current_category:
                self._process_links(tr, current_category, parsed_data)
        
        return GroupIndex(url=url, data=parsed_data)
    
    def _is_category_row(self, tr: BeautifulSoup) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºåˆ†é¡æ¨™é¡Œè¡Œ"""
        return bool(tr.find("span") and not tr.find("a"))
    
    def _process_links(
        self,
        tr: BeautifulSoup,
        category: str,
        data: Dict[str, Dict[str, str]]
    ) -> None:
        """è™•ç†è¡¨æ ¼è¡Œä¸­çš„é€£çµ"""
        for a in tr.find_all("a"):
            link = a.get("href")
            if not link:
                continue
                
            text = a.text.strip()
            if text.isdigit():
                # ç­ç´šä»£ç¢¼
                data[category][text] = link
            else:
                # æ•™å¸«åç¨±æˆ–å…¶ä»–
                clean_text = self._clean_text(text)
                if clean_text:
                    data[category][clean_text] = link
    
    def _clean_text(self, text: str) -> Optional[str]:
        """æ¸…ç†ä¸¦æ ¼å¼åŒ–æ–‡å­—"""
        # å˜—è©¦æå–ä¸­æ–‡åç¨±
        match = re.search(r'([\u4e00-\u9fa5]+)', text)
        if match:
            return match.group(1)
        
        # è™•ç†å…¶ä»–æ ¼å¼
        text = text.replace("\r", "").replace("\n", "").replace(" ", "").strip()
        if len(text) > 3:
            return text[3:].strip()
        return None


    
    async def _fetch_all_pages(self) -> Tuple[BeautifulSoup, BeautifulSoup]:
        """ä¸¦è¡Œç²å–æ‰€æœ‰éœ€è¦çš„é é¢"""

        tasks = [
            self.fetch_raw(f"{self.base_url}/{self.teacher_page}"),
            self.fetch_raw(f"{self.base_url}/{self.class_page}")
        ]
        return await asyncio.gather(*tasks)
    
    def _create_index_result(
        self,
        teacher_soup: BeautifulSoup,
        class_soup: BeautifulSoup
    ) -> IndexResult:
        """å‰µå»ºç´¢å¼•çµæœ"""
        return IndexResult(
            base_url=self.base_url,
            root=self.root,
            class_=self.parse(raw=class_soup, url=self.class_page),
            teacher=self.parse(raw=teacher_soup, url=self.teacher_page)
        )
    
    async def fetch(self, *, refresh: bool = False) -> IndexResult:
        """ç²å–å®Œæ•´çš„ç´¢å¼•è³‡æ–™
        
        Args:
            refresh: æ˜¯å¦å¼·åˆ¶æ›´æ–°å¿«å–
            
        Returns:
            IndexResult: å®Œæ•´çš„ç´¢å¼•è³‡æ–™
        """
        # ä¸¦è¡Œç²å–æ•™å¸«å’Œç­ç´šç´¢å¼•
        teacher_soup, class_soup = await self._fetch_all_pages()
        
        # è§£æè³‡æ–™ä¸¦è¿”å›çµæœ
        result = self._create_index_result(teacher_soup, class_soup)
        logger.info(f"âœ… Index[æŠ“å–]å®Œæˆ")
        return result