from typing import Optional, Dict, Tuple, List
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import re
import json
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
)
import logging

from tnfsh_timetable_core.abc.crawler_abc import BaseCrawlerABC
from tnfsh_timetable_core.index.models import (
    AllTypeIndexResult,
    DetailedIndex,
    GroupIndex,
    IndexResult,
    NewCategoryMap,
    NewGroupIndex,
    NewItemMap,
    TargetInfo,
    ReverseIndexResult
)
from tnfsh_timetable_core import TNFSHTimetableCore

core = TNFSHTimetableCore()
logger = core.get_logger(logger_level="INFO")

class IndexCrawler(BaseCrawlerABC):    
    """é¦–é ç´¢å¼•çˆ¬èŸ²ï¼Œè² è²¬çˆ¬å–ã€è§£æå’Œçµ„ç¹”èª²è¡¨ç³»çµ±çš„ç´¢å¼•è³‡è¨Š
    
    è³‡æ–™æµç¨‹ï¼š
    1. åŸå§‹è³‡æ–™ç²å–
       HTTP è«‹æ±‚ -> BeautifulSoup ç‰©ä»¶
       å‚³å…¥ï¼šURL
       è¼¸å‡ºï¼šparsed HTML
       
    2. å…§å®¹çµæ§‹è§£æ
       BeautifulSoup -> DetailedIndex
       å‚³å…¥ï¼šHTML å…§å®¹
       è¼¸å‡ºï¼šçµæ§‹åŒ–çš„åˆ†é¡æ˜ å°„
       
    3. ç´¢å¼•é—œä¿‚è™•ç†
       DetailedIndex -> å„å¼ç´¢å¼•è¡¨
       - ID å°ç…§è¡¨
       - åç¨±æ˜ å°„ï¼ˆè™•ç†é‡è¤‡ï¼‰
       - èˆŠç‰ˆæ ¼å¼è½‰æ›
       
    4. æœ€çµ‚è³‡æ–™æ•´åˆ
       æ‰€æœ‰ç´¢å¼• -> AllTypeIndexResult
    
    å…ƒä»¶ä¾è³´ï¼š
    1. ç¶²è·¯å·¥å…· (åº•å±¤)
       fetch_raw: HTTP è«‹æ±‚è™•ç†
       _fetch_all_pages: ä¸¦è¡Œè«‹æ±‚æ§åˆ¶
    
    2. è³‡æ–™æ“ä½œ (ä¸­å±¤)
       _clean_text: æ–‡å­—æ­£è¦åŒ–(ç„¡ä¾è³´)
       _is_category_row: çµæ§‹è­˜åˆ¥(ç„¡ä¾è³´)
       _parse_page: é é¢è§£æé‚è¼¯
       _derive_*: ç´¢å¼•è™•ç†å·¥å…·
    
    3. æµç¨‹æ§åˆ¶ (é ‚å±¤)
       parse: è§£ææµç¨‹èª¿åº¦
       fetch: ä¸»æµç¨‹é€²å…¥é»
       
    å‚™è¨»ï¼š
    - ä½¿ç”¨ DetailedIndex ä½œç‚ºæ ¸å¿ƒè³‡æ–™çµæ§‹
    - æ”¯æ´æ–°èˆŠæ ¼å¼è½‰æ›ï¼Œç¶­æŒç›¸å®¹æ€§
    - å¯¦ç¾ä¸¦è¡Œè«‹æ±‚ï¼Œæå‡æ•ˆèƒ½
    - è™•ç†åç¨±è¡çªï¼Œç¢ºä¿ç´¢å¼•æ­£ç¢º
    """
    
    # ====================================
    # ğŸ”§ åŸºç¤è¨­å®šï¼šç³»çµ±é…ç½®å’Œ URL å®šç¾©
    # ====================================

    # èª²è¡¨ç³»çµ±åŸºç¤ URLï¼Œå¯é€é __init__ è¦†å¯«
    DEFAULT_BASE_URL = "http://w3.tnfsh.tn.edu.tw/deanofstudies/course"
    
    # ç´¢å¼•é é¢è·¯å¾‘ï¼Œä¾æ“šå¯¦éš›éƒ¨ç½²ç’°å¢ƒå¯èƒ½éœ€è¦èª¿æ•´
    DEFAULT_TEACHER_PAGE = "_TeachIndex.html"  # æ•™å¸«ç´¢å¼•é é¢
    DEFAULT_CLASS_PAGE = "_ClassIndex.html"    # ç­ç´šç´¢å¼•é é¢
    DEFAULT_ROOT = "index.html"                # æ ¹ç›®éŒ„é é¢

    def __init__(
        self, 
        base_url: Optional[str] = None,
        root_page: Optional[str] = None,
        teacher_page: Optional[str] = None,
        class_page: Optional[str] = None
    ):
        """åˆå§‹åŒ–çˆ¬èŸ²
        
        Args:
            base_url: åŸºç¤ URL
            root_page: æ ¹ç›®éŒ„é é¢
            teacher_page: æ•™å¸«ç´¢å¼•é é¢
            class_page: ç­ç´šç´¢å¼•é é¢
        """        
        self.base_url = base_url or self.DEFAULT_BASE_URL
        self.root = root_page or self.DEFAULT_ROOT
        self.teacher_page = teacher_page or self.DEFAULT_TEACHER_PAGE
        self.class_page = class_page or self.DEFAULT_CLASS_PAGE    
    # ====================================
    # ğŸŒ ç¶²è·¯è«‹æ±‚ï¼šHTTP é€šä¿¡å’Œè³‡æ–™ç²å–
    # ====================================

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((
            aiohttp.ClientError,
            aiohttp.ServerTimeoutError,
            asyncio.TimeoutError
        )),
        before_sleep=before_sleep_log(logger, logging.WARNING)
    )
    async def fetch_raw(
        self,
        url: str,
        *,
        from_file: Optional[str] = None,
        timeout: int = 15
    ) -> BeautifulSoup:
        """ç²å–åŸå§‹ HTML å…§å®¹
        
        Args:
            url: ç›¸å° URL è·¯å¾‘
            from_file: å¯é¸çš„æœ¬åœ°æ–‡ä»¶è·¯å¾‘
            timeout: è«‹æ±‚è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            BeautifulSoup: è§£æå¾Œçš„ HTML
            
        Raises:
            aiohttp.ClientError: ç•¶ç™¼ç”Ÿç¶²è·¯éŒ¯èª¤æ™‚
            asyncio.TimeoutError: ç•¶è«‹æ±‚è¶…æ™‚æ™‚
        """
        if from_file:
            logger.debug(f"ğŸ“‚ å¾æª”æ¡ˆè®€å–ï¼š{from_file}")
            with open(from_file, 'r', encoding='utf-8') as f:
                return BeautifulSoup(f.read(), 'html.parser')
        
        logger.debug(f"ğŸŒ è«‹æ±‚ç¶²å€ï¼š{url}")
        
        async with aiohttp.ClientSession() as session:
            async with session.get(
                url,
                headers=self.get_headers(),
                timeout=aiohttp.ClientTimeout(total=timeout)
            ) as response:
                response.raise_for_status()
                content = await response.read()
                logger.debug(f"ğŸ“¥ æ”¶åˆ°å›æ‡‰ï¼š{len(content)} bytes")
                return BeautifulSoup(content, 'html.parser')

    async def _fetch_all_pages(self) -> Tuple[BeautifulSoup, BeautifulSoup]:
        """ä¸¦è¡Œç²å–æ‰€æœ‰éœ€è¦çš„é é¢
        
        Returns:
            Tuple[BeautifulSoup, BeautifulSoup]: (æ•™å¸«é é¢, ç­ç´šé é¢)
        """
        tasks = [
            self.fetch_raw(f"{self.base_url}/{self.teacher_page}"),
            self.fetch_raw(f"{self.base_url}/{self.class_page}")
        ]
        return await asyncio.gather(*tasks)

    # ====================================
    # ğŸ“ å…§å®¹è§£æï¼šHTML è§£æèˆ‡è³‡æ–™æå–
    # ====================================

    def _clean_text(self, text: str) -> Optional[str]:
        """æ¸…ç†ä¸¦æ ¼å¼åŒ–æ–‡å­—
        
        Args:
            text: åŸå§‹æ–‡å­—
            
        Returns:
            str | None: æ¸…ç†å¾Œçš„æ–‡å­—ï¼Œå¦‚æœç„¡æ³•æ¸…ç†å‰‡è¿”å› None
        """
        # å˜—è©¦æå–ä¸­æ–‡åç¨±
        match = re.search(r'([\u4e00-\u9fa5]+)', text)
        if match:
            return match.group(1)
        
        # è™•ç†å…¶ä»–æ ¼å¼
        text = text.replace("\r", "").replace("\n", "").replace(" ", "").strip()
        if len(text) > 3:
            return text[3:].strip()
        return None

    def _is_category_row(self, tr: BeautifulSoup) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºåˆ†é¡æ¨™é¡Œè¡Œ
        
        Args:
            tr: HTMLè¡¨æ ¼è¡Œ
            
        Returns:
            bool: æ˜¯å¦ç‚ºåˆ†é¡æ¨™é¡Œ
        """
        return bool(tr.find("span") and not tr.find("a"))    
    def _parse_page(
        self,
        raw: BeautifulSoup,
        is_teacher: bool
    ) -> NewCategoryMap:
        """è§£æå–®ä¸€é é¢çš„å…§å®¹ä¸¦è¿”å›åˆ†é¡æ˜ å°„
        
        Args:
            raw: BeautifulSoup ç‰©ä»¶
            is_teacher: æ˜¯å¦ç‚ºæ•™å¸«é é¢
            
        Returns:
            NewCategoryMap: åˆ†é¡åˆ° NewItemMap çš„æ˜ å°„ï¼Œå…¶ä¸­ NewItemMap æ˜¯ ID åˆ° TargetInfo çš„æ˜ å°„
        """
        # åˆå§‹åŒ–çµæœå­—å…¸ï¼šåˆ†é¡ -> (ID -> TargetInfo)
        result: Dict[str, Dict[str, TargetInfo]] = {}
        current_category = None
        
        for tr in raw.find_all("tr"):
            # è™•ç†åˆ†é¡æ¨™é¡Œ
            if self._is_category_row(tr):
                current_category = tr.find("span").text.strip()
                result[current_category] = {}
                continue
                
            # è™•ç†åˆ†é¡å…§å®¹
            if not current_category:
                continue
                
            for a in tr.find_all("a"):
                link = a.get("href")
                if not link:
                    continue
                    
                # æå–åŸºæœ¬è³‡è¨Š
                raw_name = a.text.strip()
                clean_name = self._clean_text(raw_name) if is_teacher else raw_name
                if not clean_name:
                    continue
                    
                # å»ºç«‹ç›®æ¨™è³‡è¨Š
                info = TargetInfo(
                    target=clean_name,
                    category=current_category,
                    url=link
                )
                
                # æ›´æ–°åˆ†é¡æ˜ å°„ï¼Œä½¿ç”¨ ID ä½œç‚ºéµå€¼
                result[current_category][info.id] = info
                    
        # å»ºç«‹å·¢ç‹€çµæ§‹ï¼šå…ˆå»ºç«‹æ¯å€‹åˆ†é¡çš„ NewItemMapï¼Œå†åŒ…æˆ NewCategoryMap
        category_map = {
            category: NewItemMap.model_validate(items)
            for category, items in result.items()
        }
        
        return NewCategoryMap.model_validate(category_map)

    # ====================================
    # ğŸ“Š ç´¢å¼•è™•ç†ï¼šè¡ç”Ÿç´¢å¼•çš„ç”Ÿæˆèˆ‡ç®¡ç†
    # ====================================

    def _derive_id_to_info(self, detailed: DetailedIndex) -> Dict[str, TargetInfo]:
        """å¾ detailed_index æ´¾ç”Ÿ id_to_info æ˜ å°„
        
        Args:
            detailed: è©³ç´°ç´¢å¼•
            
        Returns:
            Dict[str, TargetInfo]: ID åˆ°ç›®æ¨™è³‡è¨Šçš„æ˜ å°„
        """
        result: Dict[str, TargetInfo] = {}
        
        # å¾æ•™å¸«å’Œç­ç´šç´¢å¼•æ´¾ç”Ÿ
        for group in (detailed.teacher.data, detailed.class_.data):
            for category in group:
                for info in group[category].values():
                    result[info.id] = info
                
        return result

    def _derive_name_mappings(
        self, 
        detailed: DetailedIndex
    ) -> Tuple[Dict[str, TargetInfo], Dict[str, List[str]]]:
        """å¾ detailed_index æ´¾ç”Ÿåç¨±ç›¸é—œçš„æ˜ å°„
        
        è™•ç†é‚è¼¯ï¼š
        1. å¦‚æœåç¨±å·²åœ¨è¡çªè¡¨ä¸­ï¼Œç›´æ¥åŠ å…¥æ–°çš„ ID
        2. å¦‚æœåç¨±å·²æœ‰å”¯ä¸€æ˜ å°„ï¼Œå‰‡å°‡åŸæœ‰æ˜ å°„ç§»è‡³è¡çªè¡¨
        3. å¦‚æœåç¨±å°šæœªå‡ºç¾ï¼Œæ–°å¢å”¯ä¸€æ˜ å°„
        
        Args:
            detailed: è©³ç´°ç´¢å¼•
            
        Returns:
            Tuple[Dict[str, TargetInfo], Dict[str, List[str]]]: 
            - name_to_unique_info: å”¯ä¸€åç¨±åˆ°ç›®æ¨™è³‡è¨Šçš„æ˜ å°„
            - name_to_conflicting_ids: é‡è¤‡åç¨±åˆ° ID åˆ—è¡¨çš„æ˜ å°„
        """
        name_to_unique_info: Dict[str, TargetInfo] = {}
        name_to_conflicting_ids: Dict[str, List[str]] = {}
        
        def process_info(info: TargetInfo) -> None:
            """è™•ç†å–®ä¸€ç›®æ¨™è³‡è¨Šçš„åç¨±æ˜ å°„"""
            if info.target in name_to_conflicting_ids:
                # å¦‚æœå·²åœ¨è¡çªè¡¨ä¸­ï¼Œç›´æ¥åŠ å…¥ ID
                name_to_conflicting_ids[info.target].append(info.id)
            elif info.target in name_to_unique_info:
                # å¦‚æœå·²æœ‰å”¯ä¸€æ˜ å°„ï¼Œç§»è‡³è¡çªè¡¨
                name_to_conflicting_ids[info.target] = [
                    name_to_unique_info[info.target].id,
                    info.id
                ]
                del name_to_unique_info[info.target]
            else:
                # æ–°å¢å”¯ä¸€æ˜ å°„
                name_to_unique_info[info.target] = info
        
        # è™•ç†æ‰€æœ‰æ•™å¸«å’Œç­ç´šçš„ç›®æ¨™è³‡è¨Š
        for data in (detailed.teacher.data, detailed.class_.data):
            for category in data:
                for info in data[category].values():
                    process_info(info)
                
        return name_to_unique_info, name_to_conflicting_ids

    def _derive_old_index(self, detailed: DetailedIndex) -> IndexResult:
        """å¾ detailed_index æ´¾ç”ŸèˆŠæ ¼å¼çš„ç´¢å¼•çµæœ
        
        è™•ç†é‚è¼¯ï¼š
        1. æª¢æŸ¥æ¯å€‹ target æ˜¯å¦æœ‰è¡çª
        2. å¦‚æœæœ‰è¡çªï¼Œåœ¨ç›®æ¨™åç¨±å¾ŒåŠ ä¸Š IDï¼Œå¦‚ "ç‹å°æ˜(A01)"
        3. å¦‚æœç„¡è¡çªï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åç¨±
        
        Args:
            detailed: è©³ç´°ç´¢å¼•
            
        Returns:
            IndexResult: èˆŠæ ¼å¼çš„ç´¢å¼•çµæœ
        """
        # å…ˆå–å¾—è¡çªæ˜ å°„
        _, name_to_conflicting_ids = self._derive_name_mappings(detailed)
        
        def get_display_name(info: TargetInfo) -> str| None:
            """æ ¹æ“šè¡çªæƒ…æ³å–å¾—é¡¯ç¤ºåç¨±"""
            if info.target in name_to_conflicting_ids:
                return None
            return info.target
        
        # è½‰æ›æ•™å¸«å’Œç­ç´šè³‡æ–™
        result = {
            "teacher": {},
            "class": {}
        }
        
        # è™•ç†æ•™å¸«å’Œç­ç´šç´¢å¼•
        for group_name, group_data in (
            ("teacher", detailed.teacher.data),
            ("class", detailed.class_.data)
        ):
            for category, infos in group_data.items():
                result[group_name][category] = {
                    display_name: info.url
                    for info in infos.values()
                    if (display_name := get_display_name(info)) is not None
                }
                
        return IndexResult(
            base_url=self.base_url,
            root=self.root,
            class_=GroupIndex(url=f"{self.class_page}", data=result["class"]),
            teacher=GroupIndex(url=f"{self.teacher_page}", data=result["teacher"])
        )
    
    def _derive_old_reverse_index(self, detailed: DetailedIndex) -> ReverseIndexResult:
        """å¾ detailed_index æ´¾ç”ŸèˆŠç‰ˆåå‘ç´¢å¼•
        è™•ç†é‚è¼¯ï¼š
        1. ç›´æ¥å¾ detailed_index çš„åˆ†é¡å’Œç›®æ¨™è³‡è¨Šå»ºç«‹åå‘ç´¢å¼•
        2. æ¯å€‹ç›®æ¨™åç¨±å°æ‡‰åˆ°å…¶ URL å’Œåˆ†é¡
        Args:
            detailed: è©³ç´°ç´¢å¼•
        Returns:    
            ReverseIndexResult: åå‘ç´¢å¼•çµæœ
        """
        result: Dict[str, Dict[str, str]] = {} # {target_name: {"url": <url>, "category": <category>}}
        _, name_to_conflicting_ids = self._derive_name_mappings(detailed)
        
        def get_display_name(info: TargetInfo) -> str| None:
            """æ ¹æ“šè¡çªæƒ…æ³å–å¾—é¡¯ç¤ºåç¨±"""
            if info.target in name_to_conflicting_ids:
                return None
            return info.target

        # è™•ç†æ•™å¸«å’Œç­ç´šç´¢å¼•
        for group_name, group_data in (
            ("teacher", detailed.teacher.data),
            ("class", detailed.class_.data)
        ):
            for category, infos in group_data.items():
                for info in infos.values():
                    display_name = get_display_name(info)
                    if display_name is not None:
                        result[display_name] = {
                            "url": info.url,
                            "category": category
                        }

        return ReverseIndexResult.model_validate(result)

    # ====================================
    # ğŸ”„ ä¸»è¦æµç¨‹ï¼šæœ€çµ‚è™•ç†å’Œæµç¨‹æ§åˆ¶
    # ====================================
    
    def parse(
        self, 
        teacher_raw: BeautifulSoup, 
        class_raw: BeautifulSoup
    ) -> AllTypeIndexResult:
        """è§£ææ•™å¸«å’Œç­ç´šçš„åŸå§‹è³‡æ–™ï¼Œä¸¦å»ºç«‹å®Œæ•´çš„ç´¢å¼•çµæ§‹
        
        æ­¤æ–¹æ³•æ•´åˆäº†æ‰€æœ‰è§£æå’Œç´¢å¼•è™•ç†æµç¨‹ï¼Œæ˜¯ä¸»è¦çš„é‚è¼¯æ§åˆ¶ä¸­å¿ƒã€‚
        ä¾è³´æ‰€æœ‰å…¶ä»–è§£æå’Œç´¢å¼•è™•ç†æ–¹æ³•ã€‚
        
        æµç¨‹ï¼š
        1. ä½¿ç”¨ _parse_page è§£æåŸå§‹é é¢ï¼Œç”Ÿæˆ detailed_index
        2. ä½¿ç”¨ _derive_* æ–¹æ³•å¾ detailed_index æ´¾ç”Ÿæ‰€æœ‰å…¶ä»–ç´¢å¼•
           - ID å°ç…§è¡¨
           - åç¨±æ˜ å°„ï¼ˆè™•ç†é‡è¤‡åç¨±ï¼‰
           - èˆŠç‰ˆæ ¼å¼ç›¸å®¹

        Args:
            teacher_raw: æ•™å¸«é é¢çš„ BeautifulSoup ç‰©ä»¶
            class_raw: ç­ç´šé é¢çš„ BeautifulSoup ç‰©ä»¶
            
        Returns:
            AllTypeIndexResult: å®Œæ•´çš„ç´¢å¼•çµæœï¼ŒåŒ…å«ï¼š
            - detailed_index: è©³ç´°ç´¢å¼•è³‡è¨Š
            - id_to_info: ID æ˜ å°„
            - name_*: åç¨±ç›¸é—œæ˜ å°„
            - index: èˆŠç‰ˆæ ¼å¼ç´¢å¼•
        """
        # ç¬¬ä¸€éšæ®µï¼šè§£æåŸå§‹é é¢ï¼Œå»ºç«‹ detailed_index
        teacher_detailed = self._parse_page(teacher_raw, is_teacher=True)
        class_detailed = self._parse_page(class_raw, is_teacher=False)

        # åˆä½µç‚ºå®Œæ•´çš„ detailed_index
        detailed = DetailedIndex(
            base_url=self.base_url,
            root=self.root,
            teacher=NewGroupIndex(data=teacher_detailed, url=f"{self.teacher_page}"),
            class_=NewGroupIndex(data=class_detailed, url=f"{self.class_page}")
        )

        # ç¬¬äºŒéšæ®µï¼šå¾ detailed_index æ´¾ç”Ÿå…¶ä»–ç´¢å¼•
        id_to_info = self._derive_id_to_info(detailed)
        name_to_unique_info, name_to_conflicting_ids = self._derive_name_mappings(detailed)
        old_index = self._derive_old_index(detailed)
        old_reverse_index = self._derive_old_reverse_index(detailed) 
        
        # æœ€çµ‚éšæ®µï¼šçµ„è£å®Œæ•´çµæœ
        return AllTypeIndexResult(
            index=old_index,
            reverse_index=old_reverse_index,
            detailed_index=detailed,
            id_to_info=id_to_info,
            name_to_unique_info=name_to_unique_info,
            name_to_conflicting_ids=name_to_conflicting_ids
        )

    async def fetch(self) -> AllTypeIndexResult:
        """å–å¾—å®Œæ•´çš„ç´¢å¼•çµæœ
        
        æ­¤æ–¹æ³•æ˜¯æ•´å€‹çˆ¬èŸ²çš„æœ€é«˜å±¤ç´šå…¥å£ï¼Œæ•´åˆäº†æ‰€æœ‰åŠŸèƒ½ï¼š
        1. ç¶²è·¯è«‹æ±‚
        2. å…§å®¹è§£æ
        3. ç´¢å¼•è™•ç†
        
        ä¾è³´éˆï¼š
        - _fetch_all_pages (ç¶²è·¯è«‹æ±‚)
        - parse (è§£æå’Œç´¢å¼•)
          |- _parse_page (é é¢è§£æ)
          |- _derive_* (ç´¢å¼•è™•ç†)
        
        Returns:
            AllTypeIndexResult: å®Œæ•´çš„ç´¢å¼•çµæœ
        """
        # ç¬¬ä¸€éšæ®µï¼šä¸¦è¡Œç²å–æ•™å¸«å’Œç­ç´šç´¢å¼•é é¢
        teacher_soup, class_soup = await self._fetch_all_pages()
        
        # ç¬¬äºŒéšæ®µï¼šè§£æä¸¦å»ºç«‹å®Œæ•´çš„ç´¢å¼•çµæ§‹
        result = self.parse(teacher_raw=teacher_soup, class_raw=class_soup)
        logger.info("âœ… Index[æŠ“å–]å®Œæˆ")
        return result
    
if __name__ == "__main__":
    # æ¸¬è©¦ç”¨ä¾‹ï¼šç›´æ¥é‹è¡Œçˆ¬èŸ²
    async def main():
        crawler = IndexCrawler()
        result = await crawler.fetch()
        with open("index_result.json", "w", encoding="utf-8") as f:
            f.write(result.model_dump_json(indent=4, exclude_none=False))
        print(result.model_dump_json(indent=4, exclude_none=False))

    asyncio.run(main())