from logging import log
from typing import Optional
from datetime import datetime
from pathlib import Path
import json
from tnfsh_timetable_core.abc.cache_abc import BaseCacheABC
from tnfsh_timetable_core.index.models import (
    IndexResult, 
    AllTypeIndexResult, 
    ReverseIndexResult,
    CacheMetadata,
    CachedIndexResult
)
from tnfsh_timetable_core.index.crawler import IndexCrawler
from tnfsh_timetable_core.utils.logger import get_logger

logger = get_logger(logger_level="INFO")

# å…¨åŸŸè¨˜æ†¶é«”å¿«å–
_memory_cache: Optional[CachedIndexResult] = None

class IndexCache(BaseCacheABC):
    """èª²è¡¨ç³»çµ±ç´¢å¼•çš„å¿«å–å¯¦ä½œï¼Œæ”¯æ´ä¸‰å±¤å¿«å–æ¶æ§‹"""
    
    def __init__(self, crawler: Optional[IndexCrawler] = None, file_path: Optional[str] = None):
        """åˆå§‹åŒ–å¿«å–ç³»çµ±
        
        Args:
            crawler: ç´¢å¼•çˆ¬èŸ²å¯¦ä¾‹ï¼Œå¦‚æœæœªæä¾›æœƒå»ºç«‹æ–°çš„å¯¦ä¾‹
            file_path: å¿«å–æª”æ¡ˆè·¯å¾‘ï¼Œå¦‚æœæœªæä¾›å‰‡ä½¿ç”¨é è¨­è·¯å¾‘
        """
        self._crawler = crawler or IndexCrawler()
        self._cache_dir = Path(__file__).resolve().parent / "cache"
        self._cache_file = self._cache_dir / "prebuilt_all_index.json" if file_path is None else Path(file_path)
        self._cache_dir.mkdir(exist_ok=True)

    async def fetch_from_memory(self, *args, **kwargs) -> Optional[CachedIndexResult]:
        """å¾å…¨åŸŸè®Šæ•¸å¿«å–å–å¾—ç´¢å¼•è³‡æ–™
        
        Returns:
            Optional[CachedIndexResult]: å¿«å–çš„ç´¢å¼•è³‡æ–™ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡ç‚º None
        """
        global _memory_cache
        if _memory_cache is not None:
            logger.debug("âœ¨ å¾å…¨åŸŸè¨˜æ†¶é«”å¿«å–å–å¾—Index")
            return _memory_cache
        return None

    async def save_to_memory(self, data: CachedIndexResult, *args, **kwargs) -> None:
        """å„²å­˜ç´¢å¼•è³‡æ–™åˆ°å…¨åŸŸè®Šæ•¸å¿«å–
        
        Args:
            data: è¦å„²å­˜çš„ç´¢å¼•è³‡æ–™
        """
        global _memory_cache
        _memory_cache = data
        logger.debug("âœ¨ å·²æ›´æ–°Indexçš„å…¨åŸŸè¨˜æ†¶é«”å¿«å–")

    async def fetch_from_file(self, *args, **kwargs) -> Optional[CachedIndexResult]:
        """å¾æª”æ¡ˆå¿«å–å–å¾—ç´¢å¼•è³‡æ–™
        
        Returns:
            Optional[CachedIndexResult]: å¿«å–çš„ç´¢å¼•è³‡æ–™ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è®€å–å¤±æ•—å‰‡ç‚º None
        """
        try:
            if not self._cache_file.exists():
                return None
                
            with open(self._cache_file, encoding="utf-8") as f:
                data = json.load(f)
                result = CachedIndexResult.model_validate(data)
                logger.debug("ğŸ’¾ å¾æª”æ¡ˆè¼‰å…¥Indexå¿«å–")
                return result
        except Exception as e:
            logger.error(f"è®€å–å¿«å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    async def save_to_file(self, data: CachedIndexResult, *args, **kwargs) -> None:
        """å„²å­˜ç´¢å¼•è³‡æ–™åˆ°æª”æ¡ˆå¿«å–
        
        Args:
            data: è¦å„²å­˜çš„ç´¢å¼•è³‡æ–™
        """
        try:
            with open(self._cache_file, "w", encoding="utf-8") as f:
                json_data = data.model_dump_json(indent=4)
                f.write(json_data)
                logger.debug("ğŸ’¾ å·²æ›´æ–°Indexæª”æ¡ˆå¿«å–")
        except Exception as e:
            logger.error(f"å„²å­˜å¿«å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise

    def _create_reverse_index(self, index: IndexResult) -> ReverseIndexResult:
        """å¾æ­£å‘ç´¢å¼•å»ºç«‹åå‘ç´¢å¼•
        
        Args:
            index: ç´¢å¼•è³‡æ–™
            
        Returns:
            ReverseIndexResult: åå‘ç´¢å¼•è³‡æ–™
        """
        reverse_data = {}
        
        # è™•ç†æ•™å¸«è³‡æ–™
        for category, items in index.teacher.data.items():
            for name, url in items.items():
                reverse_data[name] = {"url": url, "category": category}
                
        # è™•ç†ç­ç´šè³‡æ–™
        for category, items in index.class_.data.items():
            for code, url in items.items():
                reverse_data[code] = {"url": url, "category": category}
                
        return ReverseIndexResult.model_validate(reverse_data)

    async def fetch_from_source(self, *args, **kwargs) -> CachedIndexResult:
        """å¾ç¶²è·¯ä¾†æºå–å¾—æœ€æ–°çš„ç´¢å¼•è³‡æ–™
        
        Returns:
            CachedIndexResult: åŒ…å«æ­£å‘å’Œåå‘ç´¢å¼•çš„å®Œæ•´ç´¢å¼•è³‡æ–™
        """
        logger.info("ğŸŒ å¾ç¶²è·¯æŠ“å–Index")
        index = await self._crawler.fetch(refresh=True)
        reverse_index = self._create_reverse_index(index)
        
        return CachedIndexResult(
            metadata=CacheMetadata(cache_fetch_at=datetime.now()),
            data=AllTypeIndexResult(
                index=index,
                reverse_index=reverse_index
            )
        )

    async def fetch(self, *, refresh: bool = False, **kwargs) -> CachedIndexResult:
        """æ™ºèƒ½ç²å–ç´¢å¼•è³‡æ–™ï¼Œè‡ªå‹•è™•ç†ä¸‰å±¤å¿«å–
        
        Args:
            refresh: æ˜¯å¦å¼·åˆ¶æ›´æ–°å¿«å–
            **kwargs: å‚³éçµ¦åº•å±¤æ–¹æ³•çš„é¡å¤–åƒæ•¸
            
        Returns:
            CachedIndexResult: å®Œæ•´çš„ç´¢å¼•è³‡æ–™
        """
        return await super().fetch(refresh=refresh, **kwargs)
